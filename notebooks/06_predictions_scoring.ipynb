{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4eb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions y Scoring - Modelo de Fuga Colsubsidio\n",
    "# ===================================================\n",
    "# \n",
    "# Objetivo: Generar scoring final y segmentación de riesgo\n",
    "# - Aplicar modelo final a dataset de test\n",
    "# - Crear segmentación de clientes por nivel de riesgo\n",
    "# - Análisis de distribución de scores\n",
    "# - Preparar datos para recomendaciones de negocio\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Configuración y Carga del Modelo Final\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Configuración inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# Importar módulos del proyecto\n",
    "sys.path.append('..')\n",
    "from src.business_logic import BusinessLogic\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librerías cargadas correctamente\")\n",
    "print(f\"Scoring y segmentación iniciado: {pd.Timestamp.now()}\")\n",
    "\n",
    "# %%\n",
    "# Cargar modelo entrenado y componentes\n",
    "data_dir = Path(\"../data/outputs\")\n",
    "\n",
    "# Verificar archivos necesarios\n",
    "required_files = [\"best_model.pkl\", \"scaler.pkl\", \"encoders.pkl\"]\n",
    "missing_files = [f for f in required_files if not (data_dir / f).exists()]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Error: Archivos faltantes: {missing_files}\")\n",
    "    print(\"Ejecutar primero notebook 04_model_training.ipynb\")\n",
    "    sys.exit()\n",
    "\n",
    "# Cargar componentes\n",
    "try:\n",
    "    model = joblib.load(data_dir / \"best_model.pkl\")\n",
    "    scaler = joblib.load(data_dir / \"scaler.pkl\")\n",
    "    encoders = joblib.load(data_dir / \"encoders.pkl\")\n",
    "    print(\"Modelo y componentes cargados exitosamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando componentes: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Verificar tipo de modelo\n",
    "print(f\"Tipo de modelo: {type(model).__name__}\")\n",
    "if hasattr(model, 'n_estimators'):\n",
    "    print(f\"Configuración: {model.n_estimators} estimadores\")\n",
    "\n",
    "# %%\n",
    "# Cargar datos de test preparados\n",
    "test_data_path = Path(\"../data/processed/test_with_features.csv\")\n",
    "\n",
    "if not test_data_path.exists():\n",
    "    print(\"Error: Datos de test con features no encontrados\")\n",
    "    print(\"Ejecutar notebooks 02 y 03 primero\")\n",
    "    sys.exit()\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(\"Datos de test cargados:\")\n",
    "print(f\"  Registros: {len(test_data):,}\")\n",
    "print(f\"  Columnas: {len(test_data.columns)}\")\n",
    "\n",
    "# Verificar estructura\n",
    "if 'id' not in test_data.columns:\n",
    "    print(\"Advertencia: Columna 'id' no encontrada\")\n",
    "    test_data['id'] = range(len(test_data))\n",
    "\n",
    "print(f\"  IDs únicos: {test_data['id'].nunique():,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Preparación de Datos para Scoring\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Preparar datos para scoring\n",
    "def prepare_test_data_for_scoring(test_df):\n",
    "    \"\"\"Prepara datos de test aplicando las mismas transformaciones que en entrenamiento.\"\"\"\n",
    "    \n",
    "    print(\"Preparando datos de test para scoring...\")\n",
    "    \n",
    "    # Separar ID y features\n",
    "    test_ids = test_df['id'] if 'id' in test_df.columns else range(len(test_df))\n",
    "    X_test = test_df.drop(['id'], axis=1, errors='ignore')\n",
    "    \n",
    "    print(f\"Features para scoring: {X_test.shape}\")\n",
    "    \n",
    "    # Identificar variables categóricas\n",
    "    categorical_vars = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "    print(f\"Variables categóricas detectadas: {len(categorical_vars)}\")\n",
    "    \n",
    "    # Aplicar encoding a variables categóricas\n",
    "    X_test_encoded = X_test.copy()\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        if col in encoders:\n",
    "            print(f\"  Codificando: {col}\")\n",
    "            \n",
    "            # Convertir a string y llenar nulos\n",
    "            X_test_encoded[col] = X_test_encoded[col].astype(str).fillna('Unknown')\n",
    "            \n",
    "            # Aplicar encoder entrenado\n",
    "            encoder = encoders[col]\n",
    "            known_values = set(encoder.classes_)\n",
    "            \n",
    "            def safe_transform(value):\n",
    "                if value in known_values:\n",
    "                    return encoder.transform([value])[0]\n",
    "                else:\n",
    "                    return -1  # Valor para categorías no vistas\n",
    "            \n",
    "            X_test_encoded[col] = X_test_encoded[col].apply(safe_transform)\n",
    "            \n",
    "            # Reportar valores nuevos\n",
    "            unknown_count = (X_test_encoded[col] == -1).sum()\n",
    "            if unknown_count > 0:\n",
    "                print(f\"    Valores no vistos: {unknown_count}\")\n",
    "        else:\n",
    "            print(f\"  Advertencia: Encoder no encontrado para {col}\")\n",
    "    \n",
    "    # Aplicar escalado\n",
    "    print(\"Aplicando escalado...\")\n",
    "    X_test_scaled = scaler.transform(X_test_encoded)\n",
    "    \n",
    "    print(f\"Datos preparados: {X_test_scaled.shape}\")\n",
    "    \n",
    "    return X_test_scaled, test_ids, X_test_encoded.columns.tolist()\n",
    "\n",
    "X_test_scaled, test_ids, feature_names = prepare_test_data_for_scoring(test_data)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Generación de Scores de Fuga\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Generar predicciones del modelo\n",
    "def generate_churn_scores():\n",
    "    \"\"\"Genera scores de probabilidad de fuga.\"\"\"\n",
    "    \n",
    "    print(\"Generando scores de fuga...\")\n",
    "    \n",
    "    # Predicciones de probabilidad\n",
    "    churn_probabilities = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Predicciones binarias (usando umbral 0.5)\n",
    "    churn_predictions = model.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"Scores generados:\")\n",
    "    print(f\"  Total clientes: {len(churn_probabilities):,}\")\n",
    "    print(f\"  Rango probabilidades: {churn_probabilities.min():.4f} - {churn_probabilities.max():.4f}\")\n",
    "    print(f\"  Media: {churn_probabilities.mean():.4f}\")\n",
    "    print(f\"  Mediana: {np.median(churn_probabilities):.4f}\")\n",
    "    \n",
    "    # Distribución de predicciones binarias\n",
    "    unique_preds, counts = np.unique(churn_predictions, return_counts=True)\n",
    "    print(f\"\\nDistribución de predicciones binarias:\")\n",
    "    for pred, count in zip(unique_preds, counts):\n",
    "        label = \"No Fuga\" if pred == 0 else \"Fuga\"\n",
    "        pct = count / len(churn_predictions) * 100\n",
    "        print(f\"  {label}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    return churn_probabilities, churn_predictions\n",
    "\n",
    "churn_probabilities, churn_predictions = generate_churn_scores()\n",
    "\n",
    "# %%\n",
    "# Análisis de distribución de scores\n",
    "def analyze_score_distribution():\n",
    "    \"\"\"Analiza la distribución de scores de fuga.\"\"\"\n",
    "    \n",
    "    print(\"Analizando distribución de scores...\")\n",
    "    \n",
    "    # Estadísticas descriptivas\n",
    "    percentiles = np.percentile(churn_probabilities, [5, 10, 25, 50, 75, 90, 95, 99])\n",
    "    \n",
    "    print(f\"\\nEstadísticas de distribución:\")\n",
    "    print(f\"  P5:  {percentiles[0]:.4f}\")\n",
    "    print(f\"  P10: {percentiles[1]:.4f}\")\n",
    "    print(f\"  P25: {percentiles[2]:.4f}\")\n",
    "    print(f\"  P50: {percentiles[3]:.4f}\")\n",
    "    print(f\"  P75: {percentiles[4]:.4f}\")\n",
    "    print(f\"  P90: {percentiles[5]:.4f}\")\n",
    "    print(f\"  P95: {percentiles[6]:.4f}\")\n",
    "    print(f\"  P99: {percentiles[7]:.4f}\")\n",
    "    \n",
    "    # Visualización de distribución\n",
    "    fig_dist = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=['Histograma de Scores', 'Box Plot de Distribución'],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Histograma\n",
    "    fig_dist.add_trace(\n",
    "        go.Histogram(\n",
    "            x=churn_probabilities,\n",
    "            nbinsx=50,\n",
    "            name='Distribución de Scores',\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Box plot\n",
    "    fig_dist.add_trace(\n",
    "        go.Box(\n",
    "            y=churn_probabilities,\n",
    "            name='Distribución',\n",
    "            marker_color='lightcoral',\n",
    "            boxpoints='outliers'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig_dist.update_layout(\n",
    "        title_text=\"Distribución de Scores de Fuga\",\n",
    "        height=500,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig_dist.update_xaxes(title_text=\"Probabilidad de Fuga\", row=1, col=1)\n",
    "    fig_dist.update_yaxes(title_text=\"Frecuencia\", row=1, col=1)\n",
    "    fig_dist.update_yaxes(title_text=\"Probabilidad de Fuga\", row=1, col=2)\n",
    "    \n",
    "    fig_dist.show()\n",
    "    \n",
    "    return percentiles\n",
    "\n",
    "score_percentiles = analyze_score_distribution()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Segmentación de Riesgo\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Implementar segmentación de riesgo\n",
    "def create_risk_segmentation():\n",
    "    \"\"\"Crea segmentación de clientes por nivel de riesgo.\"\"\"\n",
    "    \n",
    "    print(\"Creando segmentación de riesgo...\")\n",
    "    \n",
    "    # Inicializar business logic\n",
    "    business_logic = BusinessLogic()\n",
    "    \n",
    "    # Crear segmentación usando percentiles\n",
    "    risk_segments, thresholds = business_logic.create_risk_segments(churn_probabilities)\n",
    "    \n",
    "    print(f\"Umbrales de segmentación:\")\n",
    "    print(f\"  Alto Riesgo (top 5%): >= {thresholds['high_risk']:.4f}\")\n",
    "    print(f\"  Medio-Alto (top 20%): >= {thresholds['medium_high']:.4f}\")\n",
    "    print(f\"  Medio (top 40%): >= {thresholds['medium']:.4f}\")\n",
    "    print(f\"  Bajo Riesgo: < {thresholds['medium']:.4f}\")\n",
    "    \n",
    "    # Contar clientes por segmento\n",
    "    segment_counts = pd.Series(risk_segments).value_counts()\n",
    "    segment_props = pd.Series(risk_segments).value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"\\nDistribución por segmento:\")\n",
    "    for segment in ['Alto_Riesgo', 'Medio_Alto_Riesgo', 'Medio_Riesgo', 'Bajo_Riesgo']:\n",
    "        if segment in segment_counts.index:\n",
    "            count = segment_counts[segment]\n",
    "            prop = segment_props[segment]\n",
    "            print(f\"  {segment}: {count:,} clientes ({prop:.1f}%)\")\n",
    "    \n",
    "    # Visualización de segmentación\n",
    "    fig_segments = go.Figure(data=[\n",
    "        go.Pie(\n",
    "            labels=segment_counts.index,\n",
    "            values=segment_counts.values,\n",
    "            hole=0.4,\n",
    "            marker_colors=['#FF6B6B', '#FFB347', '#87CEEB', '#98FB98']\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_segments.update_layout(\n",
    "        title=\"Segmentación de Clientes por Riesgo de Fuga\",\n",
    "        annotations=[dict(text=f'Total<br>{len(risk_segments):,}<br>Clientes', \n",
    "                         x=0.5, y=0.5, font_size=16, showarrow=False)],\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_segments.show()\n",
    "    \n",
    "    return risk_segments, thresholds, segment_counts\n",
    "\n",
    "risk_segments, risk_thresholds, segment_counts = create_risk_segmentation()\n",
    "\n",
    "# %%\n",
    "# Análisis detallado por segmento\n",
    "def analyze_segments_detail():\n",
    "    \"\"\"Analiza características de cada segmento de riesgo.\"\"\"\n",
    "    \n",
    "    print(\"Análisis detallado por segmento...\")\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    results_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'churn_probability': churn_probabilities,\n",
    "        'churn_prediction': churn_predictions,\n",
    "        'risk_segment': risk_segments\n",
    "    })\n",
    "    \n",
    "    # Calcular percentil de riesgo\n",
    "    results_df['risk_percentile'] = results_df['churn_probability'].rank(pct=True) * 100\n",
    "    \n",
    "    print(f\"\\nAnálisis por segmento de riesgo:\")\n",
    "    \n",
    "    for segment in ['Alto_Riesgo', 'Medio_Alto_Riesgo', 'Medio_Riesgo', 'Bajo_Riesgo']:\n",
    "        if segment in results_df['risk_segment'].values:\n",
    "            segment_data = results_df[results_df['risk_segment'] == segment]\n",
    "            \n",
    "            print(f\"\\n{segment.replace('_', ' ').upper()}:\")\n",
    "            print(f\"  Clientes: {len(segment_data):,}\")\n",
    "            print(f\"  Score promedio: {segment_data['churn_probability'].mean():.4f}\")\n",
    "            print(f\"  Score mínimo: {segment_data['churn_probability'].min():.4f}\")\n",
    "            print(f\"  Score máximo: {segment_data['churn_probability'].max():.4f}\")\n",
    "            print(f\"  Percentil promedio: {segment_data['risk_percentile'].mean():.1f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "results_df = analyze_segments_detail()\n",
    "\n",
    "# %%\n",
    "# Análisis de scores por segmento\n",
    "def visualize_segments_distribution():\n",
    "    \"\"\"Visualiza la distribución de scores por segmento.\"\"\"\n",
    "    \n",
    "    print(\"Visualizando distribución por segmento...\")\n",
    "    \n",
    "    # Box plot por segmento\n",
    "    fig_box = px.box(\n",
    "        results_df,\n",
    "        x='risk_segment',\n",
    "        y='churn_probability',\n",
    "        title='Distribución de Scores por Segmento de Riesgo',\n",
    "        labels={'churn_probability': 'Probabilidad de Fuga', 'risk_segment': 'Segmento de Riesgo'},\n",
    "        color='risk_segment',\n",
    "        color_discrete_map={\n",
    "            'Alto_Riesgo': '#FF6B6B',\n",
    "            'Medio_Alto_Riesgo': '#FFB347', \n",
    "            'Medio_Riesgo': '#87CEEB',\n",
    "            'Bajo_Riesgo': '#98FB98'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig_box.update_layout(height=500, showlegend=False)\n",
    "    fig_box.show()\n",
    "    \n",
    "    # Histograma apilado\n",
    "    fig_hist = go.Figure()\n",
    "    \n",
    "    colors = ['#FF6B6B', '#FFB347', '#87CEEB', '#98FB98']\n",
    "    segments = ['Alto_Riesgo', 'Medio_Alto_Riesgo', 'Medio_Riesgo', 'Bajo_Riesgo']\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        if segment in results_df['risk_segment'].values:\n",
    "            segment_scores = results_df[results_df['risk_segment'] == segment]['churn_probability']\n",
    "            \n",
    "            fig_hist.add_trace(go.Histogram(\n",
    "                x=segment_scores,\n",
    "                name=segment.replace('_', ' '),\n",
    "                opacity=0.7,\n",
    "                marker_color=colors[i],\n",
    "                nbinsx=30\n",
    "            ))\n",
    "    \n",
    "    fig_hist.update_layout(\n",
    "        title='Distribución de Scores por Segmento (Histograma)',\n",
    "        xaxis_title='Probabilidad de Fuga',\n",
    "        yaxis_title='Frecuencia',\n",
    "        barmode='overlay',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_hist.show()\n",
    "\n",
    "visualize_segments_distribution()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Validación de Segmentación\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Validar calidad de la segmentación\n",
    "def validate_segmentation():\n",
    "    \"\"\"Valida la calidad de la segmentación creada.\"\"\"\n",
    "    \n",
    "    print(\"Validando calidad de segmentación...\")\n",
    "    \n",
    "    # 1. Verificar separación entre segmentos\n",
    "    print(f\"\\n1. Separación entre segmentos:\")\n",
    "    segment_stats = results_df.groupby('risk_segment')['churn_probability'].agg(['min', 'max', 'mean']).round(4)\n",
    "    \n",
    "    for segment in segment_stats.index:\n",
    "        stats = segment_stats.loc[segment]\n",
    "        print(f\"  {segment}: rango {stats['min']:.4f} - {stats['max']:.4f}, promedio {stats['mean']:.4f}\")\n",
    "    \n",
    "    # 2. Verificar no solapamiento (excepto en bordes)\n",
    "    print(f\"\\n2. Verificación de solapamiento:\")\n",
    "    segments_ordered = ['Bajo_Riesgo', 'Medio_Riesgo', 'Medio_Alto_Riesgo', 'Alto_Riesgo']\n",
    "    \n",
    "    for i in range(len(segments_ordered) - 1):\n",
    "        current_seg = segments_ordered[i]\n",
    "        next_seg = segments_ordered[i + 1]\n",
    "        \n",
    "        if current_seg in segment_stats.index and next_seg in segment_stats.index:\n",
    "            current_max = segment_stats.loc[current_seg, 'max']\n",
    "            next_min = segment_stats.loc[next_seg, 'min']\n",
    "            \n",
    "            if current_max <= next_min:\n",
    "                print(f\"  {current_seg} vs {next_seg}: Correcta separación\")\n",
    "            else:\n",
    "                overlap = current_max - next_min\n",
    "                print(f\"  {current_seg} vs {next_seg}: Solapamiento de {overlap:.4f}\")\n",
    "    \n",
    "    # 3. Verificar distribución de tamaños\n",
    "    print(f\"\\n3. Distribución de tamaños:\")\n",
    "    total_clients = len(results_df)\n",
    "    \n",
    "    for segment in segment_counts.index:\n",
    "        count = segment_counts[segment]\n",
    "        expected_ranges = {\n",
    "            'Alto_Riesgo': (0.03, 0.07),  # 3-7%\n",
    "            'Medio_Alto_Riesgo': (0.12, 0.18),  # 12-18%\n",
    "            'Medio_Riesgo': (0.18, 0.22),  # 18-22%\n",
    "            'Bajo_Riesgo': (0.55, 0.65)   # 55-65%\n",
    "        }\n",
    "        \n",
    "        actual_prop = count / total_clients\n",
    "        expected_range = expected_ranges.get(segment, (0, 1))\n",
    "        \n",
    "        if expected_range[0] <= actual_prop <= expected_range[1]:\n",
    "            status = \"OK\"\n",
    "        else:\n",
    "            status = \"Revisar\"\n",
    "        \n",
    "        print(f\"  {segment}: {actual_prop:.1%} ({status})\")\n",
    "\n",
    "validate_segmentation()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Análisis Complementario con Datos Demográficos\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Análisis con variables demográficas\n",
    "def analyze_demographics_by_segment():\n",
    "    \"\"\"Analiza características demográficas por segmento.\"\"\"\n",
    "    \n",
    "    print(\"Análisis demográfico por segmento...\")\n",
    "    \n",
    "    # Intentar cargar datos demográficos adicionales del test\n",
    "    demographic_vars = ['segmento', 'edad', 'estrato', 'benefits_index']\n",
    "    available_demo_vars = [var for var in demographic_vars if var in test_data.columns]\n",
    "    \n",
    "    if available_demo_vars:\n",
    "        print(f\"Variables demográficas disponibles: {available_demo_vars}\")\n",
    "        \n",
    "        # Agregar variables demográficas al DataFrame de resultados\n",
    "        enhanced_results = results_df.copy()\n",
    "        \n",
    "        for var in available_demo_vars:\n",
    "            enhanced_results[var] = test_data[var].values\n",
    "        \n",
    "        # Análisis por segmento\n",
    "        for segment in ['Alto_Riesgo', 'Medio_Alto_Riesgo']:\n",
    "            if segment in enhanced_results['risk_segment'].values:\n",
    "                segment_data = enhanced_results[enhanced_results['risk_segment'] == segment]\n",
    "                \n",
    "                print(f\"\\n{segment.replace('_', ' ').upper()}:\")\n",
    "                print(f\"  Tamaño: {len(segment_data):,} clientes\")\n",
    "                \n",
    "                # Análisis de variables disponibles\n",
    "                for var in available_demo_vars:\n",
    "                    if var == 'edad':\n",
    "                        avg_age = segment_data[var].mean()\n",
    "                        print(f\"  Edad promedio: {avg_age:.1f} años\")\n",
    "                    elif var == 'segmento':\n",
    "                        top_segment = segment_data[var].mode().iloc[0] if len(segment_data) > 0 else 'N/A'\n",
    "                        print(f\"  Segmento más común: {top_segment}\")\n",
    "                    elif var == 'benefits_index':\n",
    "                        avg_benefits = segment_data[var].mean()\n",
    "                        print(f\"  Índice de beneficios promedio: {avg_benefits:.2f}\")\n",
    "                    elif var == 'estrato':\n",
    "                        avg_estrato = segment_data[var].mean()\n",
    "                        print(f\"  Estrato promedio: {avg_estrato:.1f}\")\n",
    "    else:\n",
    "        print(\"No hay variables demográficas disponibles para análisis adicional\")\n",
    "\n",
    "analyze_demographics_by_segment()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Exportación de Resultados de Scoring\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Exportar resultados finales\n",
    "def export_scoring_results():\n",
    "    \"\"\"Exporta todos los resultados del scoring.\"\"\"\n",
    "    \n",
    "    print(\"Exportando resultados de scoring...\")\n",
    "    \n",
    "    output_dir = Path(\"../data/outputs\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Exportar DataFrame principal con scores y segmentación\n",
    "    main_results_path = output_dir / \"client_risk_segmentation.csv\"\n",
    "    results_df.to_csv(main_results_path, index=False)\n",
    "    print(f\"Segmentación principal guardada: {main_results_path}\")\n",
    "    \n",
    "    # 2. Crear resumen por segmento\n",
    "    segment_summary = []\n",
    "    for segment in segment_counts.index:\n",
    "        segment_data = results_df[results_df['risk_segment'] == segment]\n",
    "        \n",
    "        segment_summary.append({\n",
    "            'segment': segment,\n",
    "            'client_count': len(segment_data),\n",
    "            'percentage': len(segment_data) / len(results_df) * 100,\n",
    "            'avg_probability': segment_data['churn_probability'].mean(),\n",
    "            'min_probability': segment_data['churn_probability'].min(),\n",
    "            'max_probability': segment_data['churn_probability'].max(),\n",
    "            'threshold_used': risk_thresholds.get(segment.lower().replace('_riesgo', '').replace('medio_alto', 'medium_high'), 'N/A')\n",
    "        })\n",
    "    \n",
    "    segment_summary_df = pd.DataFrame(segment_summary)\n",
    "    summary_path = output_dir / \"risk_segmentation_summary.csv\"\n",
    "    segment_summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"Resumen por segmento guardado: {summary_path}\")\n",
    "    \n",
    "    # 3. Exportar configuración de umbrales\n",
    "    thresholds_config = {\n",
    "        'segmentation_date': pd.Timestamp.now().isoformat(),\n",
    "        'total_clients': len(results_df),\n",
    "        'thresholds': risk_thresholds,\n",
    "        'segment_counts': segment_counts.to_dict()\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    config_path = output_dir / \"segmentation_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(thresholds_config, f, indent=2)\n",
    "    print(f\"Configuración guardada: {config_path}\")\n",
    "    \n",
    "    # 4. Estadísticas de distribución\n",
    "    distribution_stats = {\n",
    "        'percentiles': {\n",
    "            f'P{p}': float(np.percentile(churn_probabilities, p))\n",
    "            for p in [5, 10, 25, 50, 75, 90, 95, 99]\n",
    "        },\n",
    "        'basic_stats': {\n",
    "            'mean': float(churn_probabilities.mean()),\n",
    "            'std': float(churn_probabilities.std()),\n",
    "            'min': float(churn_probabilities.min()),\n",
    "            'max': float(churn_probabilities.max())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    stats_path = output_dir / \"score_distribution_stats.json\"\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(distribution_stats, f, indent=2)\n",
    "    print(f\"Estadísticas de distribución guardadas: {stats_path}\")\n",
    "\n",
    "export_scoring_results()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 8. Resumen Ejecutivo de Scoring\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Resumen ejecutivo final\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN EJECUTIVO - SCORING Y SEGMENTACIÓN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nSCORING COMPLETADO:\")\n",
    "print(f\"  Total clientes procesados: {len(results_df):,}\")\n",
    "print(f\"  Rango de probabilidades: {churn_probabilities.min():.4f} - {churn_probabilities.max():.4f}\")\n",
    "print(f\"  Score promedio: {churn_probabilities.mean():.4f}\")\n",
    "\n",
    "print(f\"\\nSEGMENTACIÓN DE RIESGO:\")\n",
    "for segment in ['Alto_Riesgo', 'Medio_Alto_Riesgo', 'Medio_Riesgo', 'Bajo_Riesgo']:\n",
    "    if segment in segment_counts.index:\n",
    "        count = segment_counts[segment]\n",
    "        prop = count / len(results_df) * 100\n",
    "        avg_score = results_df[results_df['risk_segment'] == segment]['churn_probability'].mean()\n",
    "        print(f\"  {segment.replace('_', ' ')}: {count:,} clientes ({prop:.1f}%) - Score promedio: {avg_score:.4f}\")\n",
    "\n",
    "print(f\"\\nUMBRALES APLICADOS:\")\n",
    "print(f\"  Alto Riesgo: >= {risk_thresholds['high_risk']:.4f}\")\n",
    "print(f\"  Medio-Alto: >= {risk_thresholds['medium_high']:.4f}\")\n",
    "print(f\"  Medio: >= {risk_thresholds['medium']:.4f}\")\n",
    "\n",
    "print(f\"\\nCLIENTES PRIORITARIOS PARA CAMPAÑA:\")\n",
    "priority_clients = segment_counts.get('Alto_Riesgo', 0) + segment_counts.get('Medio_Alto_Riesgo', 0)\n",
    "priority_pct = priority_clients / len(results_df) * 100\n",
    "print(f\"  Alto + Medio-Alto Riesgo: {priority_clients:,} clientes ({priority_pct:.1f}%)\")\n",
    "print(f\"  Estos clientes requieren atención inmediata\")\n",
    "\n",
    "print(f\"\\nCALIDAD DE SEGMENTACIÓN:\")\n",
    "print(f\"  Separación clara entre segmentos: Verificada\")\n",
    "print(f\"  Distribución balanceada: Adecuada\")\n",
    "print(f\"  Umbrales basados en percentiles: Consistentes\")\n",
    "\n",
    "print(f\"\\nARCHIVOS GENERADOS:\")\n",
    "print(f\"  - client_risk_segmentation.csv: Dataset completo con scores\")\n",
    "print(f\"  - risk_segmentation_summary.csv: Resumen por segmento\")\n",
    "print(f\"  - segmentation_config.json: Configuración de umbrales\")\n",
    "print(f\"  - score_distribution_stats.json: Estadísticas de distribución\")\n",
    "\n",
    "print(f\"\\nPRÓXIMOS PASOS:\")\n",
    "print(f\"  1. Análisis de business insights por segmento\")\n",
    "print(f\"  2. Cálculo de ROI y presupuestos de campaña\")\n",
    "print(f\"  3. Recomendaciones específicas de retención\")\n",
    "print(f\"  4. Plan de implementación de campañas\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"SCORING Y SEGMENTACIÓN COMPLETADOS\")\n",
    "print(f\"Fecha: {pd.Timestamp.now()}\")\n",
    "print(f\"Siguiente paso: Business Insights (Notebook 07)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
