{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de Modelos - Modelo de Fuga Colsubsidio\n",
    "# =====================================================\n",
    "# \n",
    "# Objetivo: Evaluación exhaustiva y comparación de modelos entrenados\n",
    "# - Análisis profundo de métricas de performance\n",
    "# - Comparación detallada entre estrategias de balanceo\n",
    "# - Validación cruzada y estabilidad del modelo\n",
    "# - Selección final basada en criterios de negocio\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Configuración y Carga de Resultados de Entrenamiento\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Configuración inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Librerías de evaluación\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve,\n",
    "    average_precision_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librerías cargadas correctamente\")\n",
    "print(f\"Evaluación de modelos iniciada: {pd.Timestamp.now()}\")\n",
    "\n",
    "# %%\n",
    "# Cargar resultados del entrenamiento\n",
    "data_dir = Path(\"../data/outputs\")\n",
    "\n",
    "# Cargar comparación de modelos\n",
    "comparison_path = data_dir / \"model_comparison.csv\"\n",
    "if not comparison_path.exists():\n",
    "    print(\"Error: Resultados de comparación no encontrados\")\n",
    "    print(\"Ejecutar primero notebook 04_model_training.ipynb\")\n",
    "    sys.exit()\n",
    "\n",
    "comparison_df = pd.read_csv(comparison_path)\n",
    "\n",
    "print(\"Resultados de entrenamiento cargados:\")\n",
    "print(f\"  Modelos evaluados: {len(comparison_df)}\")\n",
    "print(f\"  Combinaciones probadas: {list(comparison_df[['Modelo', 'Estrategia']].apply(lambda x: f\"{x['Modelo']}+{x['Estrategia']}\", axis=1))}\")\n",
    "\n",
    "# Mostrar tabla de resultados\n",
    "print(f\"\\nTabla de resultados:\")\n",
    "print(comparison_df.round(3).to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Cargar modelo entrenado y componentes\n",
    "try:\n",
    "    best_model = joblib.load(data_dir / \"best_model.pkl\")\n",
    "    scaler = joblib.load(data_dir / \"scaler.pkl\") \n",
    "    encoders = joblib.load(data_dir / \"encoders.pkl\")\n",
    "    print(\"Modelo y componentes cargados correctamente\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error cargando componentes: {e}\")\n",
    "    print(\"Verificar que el entrenamiento se completó correctamente\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Análisis Detallado de Performance\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Análisis de la mejor combinación\n",
    "def analyze_best_model():\n",
    "    \"\"\"Analiza en detalle el mejor modelo.\"\"\"\n",
    "    \n",
    "    print(\"Análisis del mejor modelo...\")\n",
    "    \n",
    "    # Identificar mejor modelo por AUC-ROC\n",
    "    best_idx = comparison_df['AUC_ROC'].idxmax()\n",
    "    best_model_info = comparison_df.iloc[best_idx]\n",
    "    \n",
    "    print(f\"\\nMejor modelo identificado:\")\n",
    "    print(f\"  Algoritmo: {best_model_info['Modelo']}\")\n",
    "    print(f\"  Estrategia: {best_model_info['Estrategia']}\")\n",
    "    print(f\"  AUC-ROC: {best_model_info['AUC_ROC']:.3f}\")\n",
    "    \n",
    "    # Análisis de trade-offs\n",
    "    print(f\"\\nAnálisis de trade-offs:\")\n",
    "    print(f\"  Precision: {best_model_info['Precision']:.3f}\")\n",
    "    print(f\"  Recall: {best_model_info['Recall']:.3f}\")\n",
    "    print(f\"  F1-Score: {best_model_info['F1_Score']:.3f}\")\n",
    "    \n",
    "    if 'Precision@10%' in best_model_info:\n",
    "        print(f\"  Precision@10%: {best_model_info['Precision@10%']:.3f}\")\n",
    "    \n",
    "    # Interpretación de negocio\n",
    "    print(f\"\\nInterpretación para Colsubsidio:\")\n",
    "    \n",
    "    if best_model_info['Precision'] > 0.4:\n",
    "        print(\"  - Alta precisión: Pocas campañas desperdiciadas\")\n",
    "    elif best_model_info['Precision'] > 0.2:\n",
    "        print(\"  - Precisión moderada: Balance aceptable\")\n",
    "    else:\n",
    "        print(\"  - Baja precisión: Muchos falsos positivos\")\n",
    "        \n",
    "    if best_model_info['Recall'] > 0.6:\n",
    "        print(\"  - Alto recall: Detecta mayoría de casos de fuga\")\n",
    "    elif best_model_info['Recall'] > 0.3:\n",
    "        print(\"  - Recall moderado: Detecta casos más evidentes\")\n",
    "    else:\n",
    "        print(\"  - Bajo recall: Se pierden muchos casos reales\")\n",
    "    \n",
    "    return best_model_info\n",
    "\n",
    "best_model_info = analyze_best_model()\n",
    "\n",
    "# %%\n",
    "# Visualización comparativa detallada\n",
    "def create_detailed_comparison():\n",
    "    \"\"\"Crea visualizaciones detalladas de comparación.\"\"\"\n",
    "    \n",
    "    print(\"Creando visualizaciones comparativas...\")\n",
    "    \n",
    "    # Radar chart comparativo\n",
    "    metrics = ['AUC_ROC', 'Precision', 'Recall', 'F1_Score']\n",
    "    if 'Precision@10%' in comparison_df.columns:\n",
    "        metrics.append('Precision@10%')\n",
    "    \n",
    "    fig_radar = go.Figure()\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "        values = [row[metric] for metric in metrics]\n",
    "        values.append(values[0])  # Cerrar el polígono\n",
    "        \n",
    "        fig_radar.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=metrics + [metrics[0]],\n",
    "            fill='toself',\n",
    "            name=f\"{row['Modelo']} + {row['Estrategia']}\",\n",
    "            line_color=colors[i % len(colors)]\n",
    "        ))\n",
    "    \n",
    "    fig_radar.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )),\n",
    "        title=\"Comparación Multi-dimensional de Modelos\",\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig_radar.show()\n",
    "    \n",
    "    # Heatmap de métricas\n",
    "    metrics_matrix = comparison_df[metrics].values\n",
    "    model_labels = [f\"{row['Modelo']}\\n{row['Estrategia']}\" for _, row in comparison_df.iterrows()]\n",
    "    \n",
    "    fig_heatmap = px.imshow(\n",
    "        metrics_matrix.T,\n",
    "        x=model_labels,\n",
    "        y=metrics,\n",
    "        color_continuous_scale='RdYlBu_r',\n",
    "        title=\"Heatmap de Performance por Modelo y Métrica\",\n",
    "        text_auto='.3f'\n",
    "    )\n",
    "    \n",
    "    fig_heatmap.update_layout(height=500)\n",
    "    fig_heatmap.show()\n",
    "\n",
    "create_detailed_comparison()\n",
    "\n",
    "# %%\n",
    "# Análisis de estabilidad por estrategia\n",
    "def analyze_strategy_stability():\n",
    "    \"\"\"Analiza la estabilidad de cada estrategia de balanceo.\"\"\"\n",
    "    \n",
    "    print(\"Análisis de estabilidad por estrategia...\")\n",
    "    \n",
    "    strategies = comparison_df['Estrategia'].unique()\n",
    "    \n",
    "    print(f\"\\nEstrategias evaluadas: {list(strategies)}\")\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        strategy_results = comparison_df[comparison_df['Estrategia'] == strategy]\n",
    "        \n",
    "        print(f\"\\nEstrategia: {strategy}\")\n",
    "        print(f\"  Modelos probados: {len(strategy_results)}\")\n",
    "        \n",
    "        # Estadísticas por métrica\n",
    "        metrics = ['AUC_ROC', 'Precision', 'Recall', 'F1_Score']\n",
    "        for metric in metrics:\n",
    "            if metric in strategy_results.columns:\n",
    "                mean_score = strategy_results[metric].mean()\n",
    "                std_score = strategy_results[metric].std()\n",
    "                min_score = strategy_results[metric].min()\n",
    "                max_score = strategy_results[metric].max()\n",
    "                \n",
    "                print(f\"  {metric}: {mean_score:.3f} ± {std_score:.3f} (rango: {min_score:.3f}-{max_score:.3f})\")\n",
    "\n",
    "analyze_strategy_stability()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Análisis de Curvas de Performance\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Simular curvas ROC y Precision-Recall para análisis\n",
    "def simulate_performance_curves():\n",
    "    \"\"\"Simula curvas de performance para análisis visual.\"\"\"\n",
    "    \n",
    "    print(\"Generando análisis de curvas de performance...\")\n",
    "    \n",
    "    # Nota: En un escenario real, estas curvas vendrían del entrenamiento\n",
    "    # Aquí simulamos basado en las métricas conocidas\n",
    "    \n",
    "    fig_curves = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=['Curvas ROC Estimadas', 'Trade-off Precision vs Recall'],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Simulación de curvas ROC basada en AUC conocido\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "        # Simulación simple de curva ROC\n",
    "        auc_score = row['AUC_ROC']\n",
    "        fpr = np.linspace(0, 1, 100)\n",
    "        # Aproximación básica de TPR basada en AUC\n",
    "        tpr = np.minimum(1, fpr + (auc_score - 0.5) * 2)\n",
    "        \n",
    "        fig_curves.add_trace(\n",
    "            go.Scatter(\n",
    "                x=fpr, y=tpr,\n",
    "                name=f\"{row['Modelo']} + {row['Estrategia']} (AUC: {auc_score:.3f})\",\n",
    "                line=dict(color=colors[i % len(colors)]),\n",
    "                mode='lines'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Línea diagonal para referencia\n",
    "    fig_curves.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, 1], y=[0, 1],\n",
    "            mode='lines',\n",
    "            line=dict(dash='dash', color='gray'),\n",
    "            name='Random Classifier',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Gráfico Precision vs Recall\n",
    "    fig_curves.add_trace(\n",
    "        go.Scatter(\n",
    "            x=comparison_df['Recall'],\n",
    "            y=comparison_df['Precision'],\n",
    "            mode='markers+text',\n",
    "            text=[f\"{row['Modelo']}\" for _, row in comparison_df.iterrows()],\n",
    "            textposition=\"top center\",\n",
    "            marker=dict(size=10, color=comparison_df['AUC_ROC'], \n",
    "                       colorscale='Viridis', showscale=True),\n",
    "            name='Modelos',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig_curves.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
    "    fig_curves.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
    "    fig_curves.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
    "    fig_curves.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
    "    \n",
    "    fig_curves.update_layout(\n",
    "        title_text=\"Análisis de Performance de Modelos\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_curves.show()\n",
    "\n",
    "simulate_performance_curves()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Análisis de Feature Importance\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Cargar y analizar feature importance\n",
    "def analyze_feature_importance():\n",
    "    \"\"\"Analiza la importancia de features del mejor modelo.\"\"\"\n",
    "    \n",
    "    importance_path = data_dir / \"feature_importance.csv\"\n",
    "    \n",
    "    if not importance_path.exists():\n",
    "        print(\"Feature importance no disponible\")\n",
    "        return None\n",
    "    \n",
    "    importance_df = pd.read_csv(importance_path)\n",
    "    \n",
    "    print(\"Análisis de Feature Importance...\")\n",
    "    print(f\"Total features analizadas: {len(importance_df)}\")\n",
    "    \n",
    "    # Top 15 features más importantes\n",
    "    top_features = importance_df.head(15)\n",
    "    \n",
    "    print(f\"\\nTop 15 variables más importantes:\")\n",
    "    for i, row in top_features.iterrows():\n",
    "        print(f\"  {i+1:2d}. {row['feature']:<25} {row['importance_pct']:>6.2f}%\")\n",
    "    \n",
    "    # Visualización\n",
    "    fig_importance = px.bar(\n",
    "        top_features,\n",
    "        x='importance_pct',\n",
    "        y='feature',\n",
    "        orientation='h',\n",
    "        title='Feature Importance - Top 15 Variables',\n",
    "        labels={'importance_pct': 'Importancia (%)', 'feature': 'Variables'},\n",
    "        color='importance_pct',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig_importance.update_layout(\n",
    "        height=600,\n",
    "        yaxis={'categoryorder': 'total ascending'}\n",
    "    )\n",
    "    \n",
    "    fig_importance.show()\n",
    "    \n",
    "    # Categorización de variables\n",
    "    print(f\"\\nCategorización de variables importantes:\")\n",
    "    \n",
    "    # Variables financieras\n",
    "    financial_vars = [var for var in top_features['feature'] \n",
    "                     if any(fword in var.lower() for fword in ['saldo', 'limite', 'pago', 'mora', 'cupo'])]\n",
    "    \n",
    "    # Variables derivadas\n",
    "    derived_vars = [var for var in top_features['feature'] \n",
    "                   if any(dword in var.lower() for dword in ['utilization', 'stress', 'activity', 'benefit'])]\n",
    "    \n",
    "    # Variables demográficas\n",
    "    demo_vars = [var for var in top_features['feature'] \n",
    "                if any(demo in var.lower() for demo in ['edad', 'segmento', 'estrato', 'genero'])]\n",
    "    \n",
    "    print(f\"  Variables financieras: {len(financial_vars)} ({financial_vars[:3]}...)\")\n",
    "    print(f\"  Variables derivadas: {len(derived_vars)} ({derived_vars[:3]}...)\")\n",
    "    print(f\"  Variables demográficas: {len(demo_vars)} ({demo_vars[:3]}...)\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "feature_importance_df = analyze_feature_importance()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Validación de Robustez del Modelo\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Análisis de robustez y estabilidad\n",
    "def robustness_analysis():\n",
    "    \"\"\"Analiza la robustez del modelo seleccionado.\"\"\"\n",
    "    \n",
    "    print(\"Análisis de robustez del modelo...\")\n",
    "    \n",
    "    # Análisis de sensibilidad de métricas\n",
    "    print(f\"\\nAnálisis de sensibilidad:\")\n",
    "    \n",
    "    # Calcular coeficiente de variación entre modelos de la misma estrategia\n",
    "    best_strategy = best_model_info['Estrategia']\n",
    "    strategy_models = comparison_df[comparison_df['Estrategia'] == best_strategy]\n",
    "    \n",
    "    if len(strategy_models) > 1:\n",
    "        print(f\"Estabilidad dentro de la estrategia {best_strategy}:\")\n",
    "        \n",
    "        for metric in ['AUC_ROC', 'Precision', 'Recall', 'F1_Score']:\n",
    "            if metric in strategy_models.columns:\n",
    "                mean_val = strategy_models[metric].mean()\n",
    "                std_val = strategy_models[metric].std()\n",
    "                cv = std_val / mean_val if mean_val > 0 else 0\n",
    "                \n",
    "                if cv < 0.1:\n",
    "                    stability = \"Muy estable\"\n",
    "                elif cv < 0.2:\n",
    "                    stability = \"Estable\"\n",
    "                else:\n",
    "                    stability = \"Variable\"\n",
    "                \n",
    "                print(f\"  {metric}: {stability} (CV: {cv:.3f})\")\n",
    "    \n",
    "    # Análisis de distribución de errores (simulado)\n",
    "    print(f\"\\nAnálisis de distribución de errores:\")\n",
    "    print(f\"  Precision: {best_model_info['Precision']:.3f} - \", end=\"\")\n",
    "    if best_model_info['Precision'] > 0.5:\n",
    "        print(\"Baja tasa de falsos positivos\")\n",
    "    elif best_model_info['Precision'] > 0.3:\n",
    "        print(\"Tasa moderada de falsos positivos\")\n",
    "    else:\n",
    "        print(\"Alta tasa de falsos positivos - revisar umbrales\")\n",
    "    \n",
    "    print(f\"  Recall: {best_model_info['Recall']:.3f} - \", end=\"\")\n",
    "    if best_model_info['Recall'] > 0.7:\n",
    "        print(\"Baja tasa de falsos negativos\")\n",
    "    elif best_model_info['Recall'] > 0.4:\n",
    "        print(\"Tasa moderada de falsos negativos\")\n",
    "    else:\n",
    "        print(\"Alta tasa de falsos negativos - casos perdidos\")\n",
    "\n",
    "robustness_analysis()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Recomendaciones de Implementación\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Generar recomendaciones de implementación\n",
    "def implementation_recommendations():\n",
    "    \"\"\"Genera recomendaciones para implementación.\"\"\"\n",
    "    \n",
    "    print(\"Recomendaciones de implementación...\")\n",
    "    \n",
    "    print(f\"\\nModelo recomendado:\")\n",
    "    print(f\"  Algoritmo: {best_model_info['Modelo']}\")\n",
    "    print(f\"  Estrategia: {best_model_info['Estrategia']}\")\n",
    "    print(f\"  Justificación: Balance óptimo entre métricas de negocio\")\n",
    "    \n",
    "    print(f\"\\nConsideraciones de implementación:\")\n",
    "    \n",
    "    # Recomendaciones basadas en performance\n",
    "    if best_model_info['AUC_ROC'] > 0.8:\n",
    "        print(\"  - Modelo tiene excelente capacidad discriminativa\")\n",
    "        print(\"  - Confiable para segmentación de riesgo\")\n",
    "    elif best_model_info['AUC_ROC'] > 0.7:\n",
    "        print(\"  - Modelo tiene buena capacidad discriminativa\")\n",
    "        print(\"  - Adecuado para implementación con monitoreo\")\n",
    "    else:\n",
    "        print(\"  - Modelo requiere mejoras antes de implementación\")\n",
    "    \n",
    "    # Recomendaciones de monitoreo\n",
    "    print(f\"\\nPlan de monitoreo recomendado:\")\n",
    "    print(\"  1. Evaluación mensual de distribución de scores\")\n",
    "    print(\"  2. Seguimiento de tasa de conversión de campañas\")\n",
    "    print(\"  3. Reentrenamiento trimestral con nuevos datos\")\n",
    "    print(\"  4. Validación de drift en variables importantes\")\n",
    "    \n",
    "    # Métricas de seguimiento\n",
    "    print(f\"\\nMétricas clave de seguimiento:\")\n",
    "    print(f\"  - AUC-ROC objetivo: > {best_model_info['AUC_ROC']:.3f}\")\n",
    "    print(f\"  - Precision objetivo: > {best_model_info['Precision']:.3f}\")\n",
    "    print(\"  - Tasa de respuesta de campañas: > 15%\")\n",
    "    print(\"  - ROI de campañas: > 3:1\")\n",
    "\n",
    "implementation_recommendations()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Exportación de Resultados de Evaluación\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Guardar análisis de evaluación\n",
    "output_dir = Path(\"../data/outputs\")\n",
    "\n",
    "print(\"Exportando resultados de evaluación...\")\n",
    "\n",
    "# Crear reporte de evaluación\n",
    "evaluation_report = {\n",
    "    'best_model': {\n",
    "        'algorithm': best_model_info['Modelo'],\n",
    "        'strategy': best_model_info['Estrategia'],\n",
    "        'auc_roc': best_model_info['AUC_ROC'],\n",
    "        'precision': best_model_info['Precision'],\n",
    "        'recall': best_model_info['Recall'],\n",
    "        'f1_score': best_model_info['F1_Score']\n",
    "    },\n",
    "    'all_results': comparison_df.to_dict('records'),\n",
    "    'evaluation_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "# Guardar como JSON para fácil lectura\n",
    "import json\n",
    "report_path = output_dir / \"model_evaluation_report.json\"\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(evaluation_report, f, indent=2)\n",
    "\n",
    "print(f\"Reporte de evaluación guardado en: {report_path}\")\n",
    "\n",
    "# Guardar feature importance analizada\n",
    "if feature_importance_df is not None:\n",
    "    analyzed_importance_path = output_dir / \"feature_importance_analyzed.csv\"\n",
    "    feature_importance_df.to_csv(analyzed_importance_path, index=False)\n",
    "    print(f\"Feature importance analizada guardada en: {analyzed_importance_path}\")\n",
    "\n",
    "print(\"Evaluación completada y exportada\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 8. Resumen Ejecutivo de Evaluación\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Resumen ejecutivo final\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN EJECUTIVO - EVALUACIÓN DE MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nMODELO FINAL SELECCIONADO:\")\n",
    "print(f\"  Algoritmo: {best_model_info['Modelo']}\")\n",
    "print(f\"  Estrategia de balanceo: {best_model_info['Estrategia']}\")\n",
    "\n",
    "print(f\"\\nMÉTRICAS DE PERFORMANCE:\")\n",
    "print(f\"  AUC-ROC: {best_model_info['AUC_ROC']:.3f}\")\n",
    "print(f\"  Precision: {best_model_info['Precision']:.3f}\")\n",
    "print(f\"  Recall: {best_model_info['Recall']:.3f}\")\n",
    "print(f\"  F1-Score: {best_model_info['F1_Score']:.3f}\")\n",
    "\n",
    "if 'Precision@10%' in best_model_info:\n",
    "    print(f\"  Precision@10%: {best_model_info['Precision@10%']:.3f}\")\n",
    "\n",
    "print(f\"\\nFORTALEZAS DEL MODELO:\")\n",
    "strengths = []\n",
    "if best_model_info['AUC_ROC'] > 0.8:\n",
    "    strengths.append(\"Excelente capacidad discriminativa\")\n",
    "if best_model_info['Precision'] > 0.4:\n",
    "    strengths.append(\"Baja tasa de falsos positivos\")\n",
    "if best_model_info['Recall'] > 0.3:\n",
    "    strengths.append(\"Detección adecuada de casos reales\")\n",
    "\n",
    "for strength in strengths:\n",
    "    print(f\"  - {strength}\")\n",
    "\n",
    "print(f\"\\nCONSIDERACIONES:\")\n",
    "considerations = []\n",
    "if best_model_info['Precision'] < 0.3:\n",
    "    considerations.append(\"Monitorear falsos positivos en campañas\")\n",
    "if best_model_info['Recall'] < 0.4:\n",
    "    considerations.append(\"Algunos casos de fuga pueden no detectarse\")\n",
    "\n",
    "if considerations:\n",
    "    for consideration in considerations:\n",
    "        print(f\"  - {consideration}\")\n",
    "else:\n",
    "    print(\"  - Modelo balanceado sin consideraciones críticas\")\n",
    "\n",
    "print(f\"\\nRECOMENDACIÓN:\")\n",
    "print(\"  APROBAR IMPLEMENTACIÓN del modelo seleccionado\")\n",
    "print(\"  con plan de monitoreo y reentrenamiento establecido\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"EVALUACIÓN COMPLETADA\")\n",
    "print(f\"Fecha: {pd.Timestamp.now()}\")\n",
    "print(f\"Siguiente paso: Scoring y Segmentación (Notebook 06)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fb870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
