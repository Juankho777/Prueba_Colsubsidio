{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de Modelos - Modelo de Fuga Colsubsidio\n",
    "# =======================================================\n",
    "# \n",
    "# Objetivo: Entrenar y comparar modelos con diferentes estrategias de balanceo\n",
    "# - Manejo del desbalance de clases (34:1)\n",
    "# - Comparación Random Forest vs Logistic Regression\n",
    "# - Estrategias: Class Weights, Undersampling, Oversampling\n",
    "# - Validación y selección del mejor modelo\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Configuración Inicial y Carga de Datos\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Configuración inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Librerías de machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Importar módulos del proyecto\n",
    "sys.path.append('..')\n",
    "from src.model_utils import ModelTrainer\n",
    "from src.preprocessing import DataPreprocessor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Librerías cargadas correctamente\")\n",
    "print(f\"Inicio del entrenamiento: {pd.Timestamp.now()}\")\n",
    "\n",
    "# %%\n",
    "# Cargar datos con features del notebook anterior\n",
    "data_dir = Path(\"../data/processed\")\n",
    "\n",
    "train_path = data_dir / \"train_with_features.csv\"\n",
    "test_path = data_dir / \"test_with_features.csv\"\n",
    "\n",
    "if not train_path.exists() or not test_path.exists():\n",
    "    print(\"Error: Archivos con features no encontrados\")\n",
    "    print(\"Necesitas ejecutar primero el notebook 03_feature_engineering.ipynb\")\n",
    "    sys.exit()\n",
    "\n",
    "# Cargar datasets\n",
    "train_features = pd.read_csv(train_path)\n",
    "test_features = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Datos cargados:\")\n",
    "print(f\"Train: {len(train_features):,} registros x {len(train_features.columns)} columnas\")\n",
    "print(f\"Test: {len(test_features):,} registros x {len(test_features.columns)} columnas\")\n",
    "\n",
    "# Verificar target y distribución\n",
    "if 'Target' in train_features.columns:\n",
    "    target_dist = train_features['Target'].value_counts()\n",
    "    target_props = train_features['Target'].value_counts(normalize=True)\n",
    "    imbalance_ratio = target_dist[0] / target_dist[1]\n",
    "    \n",
    "    print(f\"\\nDistribución del Target:\")\n",
    "    print(f\"  No Fuga (0): {target_dist[0]:,} ({target_props[0]:.1%})\")\n",
    "    print(f\"  Fuga (1): {target_dist[1]:,} ({target_props[1]:.1%})\")\n",
    "    print(f\"  Ratio desbalance: {imbalance_ratio:.0f}:1\")\n",
    "    \n",
    "    if target_props[1] < 0.05:\n",
    "        print(\"  ALERTA: Desbalance extremo detectado\")\n",
    "else:\n",
    "    print(\"Error: Variable Target no encontrada\")\n",
    "    sys.exit()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Preparación de Datos para Modelado\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Selección y preparación de features\n",
    "def prepare_modeling_data():\n",
    "    \"\"\"Prepara datos para modelado con feature selection.\"\"\"\n",
    "    \n",
    "    print(\"Preparando datos para modelado...\")\n",
    "    \n",
    "    # Variables a excluir del modelo\n",
    "    exclude_vars = ['id', 'Target']\n",
    "    \n",
    "    # Identificar variables categóricas\n",
    "    categorical_vars = train_features.select_dtypes(include=['object']).columns\n",
    "    categorical_vars = [col for col in categorical_vars if col not in exclude_vars]\n",
    "    \n",
    "    print(f\"Variables categóricas detectadas: {len(categorical_vars)}\")\n",
    "    if categorical_vars:\n",
    "        print(f\"  Ejemplos: {categorical_vars[:3]}\")\n",
    "    \n",
    "    # Preparar X y y\n",
    "    X = train_features.drop(exclude_vars, axis=1)\n",
    "    y = train_features['Target']\n",
    "    \n",
    "    # Preparar test features\n",
    "    X_test = test_features.drop(['id'], axis=1, errors='ignore')\n",
    "    test_ids = test_features['id'] if 'id' in test_features.columns else None\n",
    "    \n",
    "    print(f\"Dimensiones preparadas:\")\n",
    "    print(f\"  X_train: {X.shape}\")\n",
    "    print(f\"  y_train: {y.shape}\")\n",
    "    print(f\"  X_test: {X_test.shape}\")\n",
    "    \n",
    "    return X, y, X_test, test_ids, categorical_vars\n",
    "\n",
    "X, y, X_test, test_ids, categorical_vars = prepare_modeling_data()\n",
    "\n",
    "# %%\n",
    "# Encoding de variables categóricas\n",
    "def encode_categorical_variables(X_train, X_test, categorical_vars):\n",
    "    \"\"\"Codifica variables categóricas usando Label Encoding.\"\"\"\n",
    "    \n",
    "    print(f\"Codificando variables categóricas...\")\n",
    "    \n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_test_encoded = X_test.copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        if col in X_train_encoded.columns:\n",
    "            print(f\"  Procesando {col}\")\n",
    "            \n",
    "            encoder = LabelEncoder()\n",
    "            \n",
    "            # Convertir a string y manejar valores nulos\n",
    "            X_train_encoded[col] = X_train_encoded[col].astype(str).fillna('Unknown')\n",
    "            X_test_encoded[col] = X_test_encoded[col].astype(str).fillna('Unknown')\n",
    "            \n",
    "            # Ajustar encoder en train\n",
    "            X_train_encoded[col] = encoder.fit_transform(X_train_encoded[col])\n",
    "            encoders[col] = encoder\n",
    "            \n",
    "            # Aplicar a test, manejando valores no vistos\n",
    "            test_values = X_test_encoded[col].unique()\n",
    "            known_values = set(encoder.classes_)\n",
    "            \n",
    "            def safe_transform(value):\n",
    "                if value in known_values:\n",
    "                    return encoder.transform([value])[0]\n",
    "                else:\n",
    "                    return -1  # Valor para categorías nuevas\n",
    "            \n",
    "            X_test_encoded[col] = X_test_encoded[col].apply(safe_transform)\n",
    "            \n",
    "            unknown_test = (X_test_encoded[col] == -1).sum()\n",
    "            if unknown_test > 0:\n",
    "                print(f\"    Valores nuevos en test: {unknown_test}\")\n",
    "    \n",
    "    print(f\"Encoding completado para {len(categorical_vars)} variables\")\n",
    "    return X_train_encoded, X_test_encoded, encoders\n",
    "\n",
    "X_encoded, X_test_encoded, encoders = encode_categorical_variables(X, X_test, categorical_vars)\n",
    "\n",
    "# %%\n",
    "# División train/validación y escalado\n",
    "def prepare_train_validation_split():\n",
    "    \"\"\"Crea división train/validación y aplica escalado.\"\"\"\n",
    "    \n",
    "    print(f\"Creando división train/validación...\")\n",
    "    \n",
    "    # División estratificada\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_encoded, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"División completada:\")\n",
    "    print(f\"  Train: {X_train.shape}\")\n",
    "    print(f\"  Validation: {X_val.shape}\")\n",
    "    \n",
    "    # Escalado de features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test_encoded)\n",
    "    \n",
    "    print(f\"Escalado aplicado con StandardScaler\")\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, scaler\n",
    "\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, scaler = prepare_train_validation_split()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Estrategias de Manejo de Desbalance\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Análisis del problema de desbalance\n",
    "def analyze_class_imbalance():\n",
    "    \"\"\"Analiza el problema de desbalance en detalle.\"\"\"\n",
    "    \n",
    "    print(\"Analizando desbalance de clases...\")\n",
    "    \n",
    "    # Estadísticas del desbalance\n",
    "    class_counts = y_train.value_counts()\n",
    "    class_props = y_train.value_counts(normalize=True) * 100\n",
    "    imbalance_ratio = class_counts[0] / class_counts[1]\n",
    "    \n",
    "    print(f\"\\nDistribución en entrenamiento:\")\n",
    "    print(f\"  Clase 0 (No Fuga): {class_counts[0]:,} ({class_props[0]:.1f}%)\")\n",
    "    print(f\"  Clase 1 (Fuga): {class_counts[1]:,} ({class_props[1]:.1f}%)\")\n",
    "    print(f\"  Ratio de desbalance: {imbalance_ratio:.0f}:1\")\n",
    "    \n",
    "    # Visualización del desbalance\n",
    "    fig_imbalance = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=['No Fuga', 'Fuga'],\n",
    "            y=[class_counts[0], class_counts[1]],\n",
    "            marker_color=['lightblue', 'lightcoral'],\n",
    "            text=[f'{class_counts[0]:,}', f'{class_counts[1]:,}'],\n",
    "            textposition='auto'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_imbalance.update_layout(\n",
    "        title='Distribución de Clases en Datos de Entrenamiento',\n",
    "        xaxis_title='Clase',\n",
    "        yaxis_title='Número de Observaciones',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig_imbalance.show()\n",
    "    \n",
    "    # Impacto del desbalance\n",
    "    print(f\"\\nImpacto del desbalance:\")\n",
    "    accuracy_if_predict_majority = class_props[0] / 100\n",
    "    print(f\"  Accuracy prediciendo siempre mayoría: {accuracy_if_predict_majority:.1%}\")\n",
    "    print(f\"  Clase minoritaria: {class_props[1]:.1f}% de los datos\")\n",
    "    \n",
    "    return {\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'minority_percentage': class_props[1],\n",
    "        'majority_count': class_counts[0],\n",
    "        'minority_count': class_counts[1]\n",
    "    }\n",
    "\n",
    "imbalance_stats = analyze_class_imbalance()\n",
    "\n",
    "# %%\n",
    "# Implementar estrategias de balanceo\n",
    "def implement_balancing_strategies():\n",
    "    \"\"\"Implementa diferentes estrategias de balanceo.\"\"\"\n",
    "    \n",
    "    print(f\"Implementando estrategias de balanceo...\")\n",
    "    \n",
    "    strategies = {}\n",
    "    \n",
    "    # 1. ESTRATEGIA: Class Weights\n",
    "    print(f\"\\n1. Calculando Class Weights...\")\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    \n",
    "    print(f\"   Pesos calculados:\")\n",
    "    print(f\"     Clase 0: {class_weight_dict[0]:.3f}\")\n",
    "    print(f\"     Clase 1: {class_weight_dict[1]:.3f}\")\n",
    "    print(f\"   Factor de penalización: {class_weight_dict[1]/class_weight_dict[0]:.1f}x\")\n",
    "    \n",
    "    strategies['class_weights'] = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'y_train': y_train,\n",
    "        'class_weight': class_weight_dict,\n",
    "        'description': 'Class Weights'\n",
    "    }\n",
    "    \n",
    "    # 2. ESTRATEGIA: Undersampling\n",
    "    print(f\"\\n2. Aplicando Undersampling...\")\n",
    "    \n",
    "    # Resetear índices\n",
    "    y_train_reset = y_train.reset_index(drop=True)\n",
    "    \n",
    "    # Obtener posiciones por clase\n",
    "    pos_class_0 = np.where(y_train_reset == 0)[0]\n",
    "    pos_class_1 = np.where(y_train_reset == 1)[0]\n",
    "    \n",
    "    # Undersampling conservador (ratio 10:1)\n",
    "    n_minority = len(pos_class_1)\n",
    "    n_majority_sample = min(n_minority * 10, len(pos_class_0))\n",
    "    \n",
    "    # Muestreo aleatorio\n",
    "    np.random.seed(42)\n",
    "    pos_class_0_sample = np.random.choice(pos_class_0, size=n_majority_sample, replace=False)\n",
    "    \n",
    "    # Combinar\n",
    "    balanced_positions = np.concatenate([pos_class_0_sample, pos_class_1])\n",
    "    np.random.shuffle(balanced_positions)\n",
    "    \n",
    "    # Crear datasets balanceados\n",
    "    X_train_under = X_train_scaled[balanced_positions]\n",
    "    y_train_under = y_train_reset.iloc[balanced_positions]\n",
    "    \n",
    "    print(f\"   Antes: {len(y_train):,} registros\")\n",
    "    print(f\"   Después: {len(y_train_under):,} registros\")\n",
    "    print(f\"   Nuevo ratio: {y_train_under.value_counts()[0]/y_train_under.value_counts()[1]:.1f}:1\")\n",
    "    \n",
    "    strategies['undersampling'] = {\n",
    "        'X_train': X_train_under,\n",
    "        'y_train': y_train_under,\n",
    "        'class_weight': None,\n",
    "        'description': 'Undersampling'\n",
    "    }\n",
    "    \n",
    "    # 3. ESTRATEGIA: Oversampling\n",
    "    print(f\"\\n3. Aplicando Oversampling...\")\n",
    "    \n",
    "    # Oversampling por duplicación\n",
    "    minority_multiplier = 3\n",
    "    \n",
    "    # Replicar clase minoritaria\n",
    "    pos_minority_replicated = np.tile(pos_class_1, minority_multiplier)\n",
    "    \n",
    "    # Combinar\n",
    "    all_positions = np.concatenate([pos_class_0, pos_minority_replicated])\n",
    "    np.random.shuffle(all_positions)\n",
    "    \n",
    "    # Crear dataset oversampled\n",
    "    X_train_over = X_train_scaled[all_positions]\n",
    "    y_train_over = y_train_reset.iloc[all_positions]\n",
    "    \n",
    "    print(f\"   Antes: {len(y_train):,} registros\")\n",
    "    print(f\"   Después: {len(y_train_over):,} registros\")\n",
    "    print(f\"   Nuevo ratio: {y_train_over.value_counts()[0]/y_train_over.value_counts()[1]:.1f}:1\")\n",
    "    \n",
    "    strategies['oversampling'] = {\n",
    "        'X_train': X_train_over,\n",
    "        'y_train': y_train_over,\n",
    "        'class_weight': None,\n",
    "        'description': 'Oversampling'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEstrategias implementadas: {list(strategies.keys())}\")\n",
    "    \n",
    "    return strategies\n",
    "\n",
    "balancing_strategies = implement_balancing_strategies()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Entrenamiento y Evaluación de Modelos\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Configuración de modelos\n",
    "def setup_models():\n",
    "    \"\"\"Configura los modelos a entrenar.\"\"\"\n",
    "    \n",
    "    print(\"Configurando modelos...\")\n",
    "    \n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'name': 'Random Forest'\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'model': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=1000,\n",
    "                solver='liblinear'\n",
    "            ),\n",
    "            'name': 'Logistic Regression'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Modelos configurados:\")\n",
    "    for key, config in models.items():\n",
    "        print(f\"  - {config['name']}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "model_configs = setup_models()\n",
    "\n",
    "# %%\n",
    "# Función de evaluación comprehensiva\n",
    "def evaluate_model_comprehensive(model, X_train, y_train, X_val, y_val, strategy_name, model_name):\n",
    "    \"\"\"Evalúa modelo con métricas comprehensivas.\"\"\"\n",
    "    \n",
    "    print(f\"\\nEvaluando: {model_name} - {strategy_name}\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Métricas básicas\n",
    "    auc_roc = roc_auc_score(y_val, y_pred_proba)\n",
    "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "    \n",
    "    # Precision at 10% (relevante para campañas)\n",
    "    top_k = int(len(y_pred_proba) * 0.1)\n",
    "    top_indices = np.argsort(y_pred_proba)[-top_k:]\n",
    "    precision_at_k = y_val.iloc[top_indices].mean()\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"   AUC-ROC: {auc_roc:.3f}\")\n",
    "    print(f\"   Precision: {precision:.3f}\")\n",
    "    print(f\"   Recall: {recall:.3f}\")\n",
    "    print(f\"   F1-Score: {f1:.3f}\")\n",
    "    print(f\"   Precision@10%: {precision_at_k:.3f}\")\n",
    "    print(f\"   Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'strategy': strategy_name,\n",
    "        'model_name': model_name,\n",
    "        'auc_roc': auc_roc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'precision_at_k': precision_at_k,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': y_pred_proba\n",
    "    }\n",
    "\n",
    "# %%\n",
    "# Entrenar todas las combinaciones\n",
    "def train_all_combinations():\n",
    "    \"\"\"Entrena todos los modelos con todas las estrategias.\"\"\"\n",
    "    \n",
    "    print(\"Entrenando todas las combinaciones...\")\n",
    "    \n",
    "    results = []\n",
    "    total_combinations = len(model_configs) * len(balancing_strategies)\n",
    "    current_combination = 0\n",
    "    \n",
    "    for model_key, model_config in model_configs.items():\n",
    "        for strategy_key, strategy_config in balancing_strategies.items():\n",
    "            current_combination += 1\n",
    "            print(f\"\\n[{current_combination}/{total_combinations}] {model_config['name']} + {strategy_config['description']}\")\n",
    "            \n",
    "            # Preparar modelo\n",
    "            model = model_config['model']\n",
    "            \n",
    "            # Aplicar class weights si corresponde\n",
    "            if strategy_config['class_weight'] is not None:\n",
    "                if hasattr(model, 'class_weight'):\n",
    "                    model.set_params(class_weight=strategy_config['class_weight'])\n",
    "            \n",
    "            # Entrenar y evaluar\n",
    "            result = evaluate_model_comprehensive(\n",
    "                model=model,\n",
    "                X_train=strategy_config['X_train'],\n",
    "                y_train=strategy_config['y_train'],\n",
    "                X_val=X_val_scaled,\n",
    "                y_val=y_val,\n",
    "                strategy_name=strategy_key,\n",
    "                model_name=model_config['name']\n",
    "            )\n",
    "            \n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar entrenamiento\n",
    "print(\"Iniciando entrenamiento completo...\")\n",
    "all_results = train_all_combinations()\n",
    "print(\"Entrenamiento completado!\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Análisis Comparativo de Resultados\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Crear tabla comparativa\n",
    "def create_comparison_table(results):\n",
    "    \"\"\"Crea tabla comparativa de resultados.\"\"\"\n",
    "    \n",
    "    print(\"Creando tabla comparativa...\")\n",
    "    \n",
    "    comparison_data = []\n",
    "    for result in results:\n",
    "        comparison_data.append({\n",
    "            'Modelo': result['model_name'],\n",
    "            'Estrategia': result['strategy'],\n",
    "            'AUC_ROC': result['auc_roc'],\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'F1_Score': result['f1_score'],\n",
    "            'Precision@10%': result['precision_at_k']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Mostrar tabla\n",
    "    print(\"\\nResultados completos:\")\n",
    "    print(comparison_df.round(3).to_string(index=False))\n",
    "    \n",
    "    # Mejores resultados por métrica\n",
    "    print(f\"\\nMejores resultados por métrica:\")\n",
    "    metrics = ['AUC_ROC', 'Precision', 'Recall', 'F1_Score', 'Precision@10%']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        best_idx = comparison_df[metric].idxmax()\n",
    "        best_result = comparison_df.iloc[best_idx]\n",
    "        print(f\"   {metric}: {best_result['Modelo']} + {best_result['Estrategia']} ({best_result[metric]:.3f})\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "comparison_df = create_comparison_table(all_results)\n",
    "\n",
    "# %%\n",
    "# Visualización comparativa\n",
    "def visualize_model_comparison():\n",
    "    \"\"\"Visualiza comparación de modelos.\"\"\"\n",
    "    \n",
    "    print(\"Creando visualizaciones...\")\n",
    "    \n",
    "    # Gráfico de barras para AUC-ROC\n",
    "    fig_auc = px.bar(\n",
    "        comparison_df,\n",
    "        x='AUC_ROC',\n",
    "        y=[f\"{row['Modelo']} + {row['Estrategia']}\" for _, row in comparison_df.iterrows()],\n",
    "        orientation='h',\n",
    "        title='Comparación AUC-ROC por Modelo y Estrategia',\n",
    "        labels={'AUC_ROC': 'AUC-ROC Score', 'y': 'Modelo + Estrategia'},\n",
    "        color='AUC_ROC',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig_auc.update_layout(\n",
    "        height=400,\n",
    "        yaxis={'categoryorder': 'total ascending'}\n",
    "    )\n",
    "    \n",
    "    fig_auc.show()\n",
    "\n",
    "visualize_model_comparison()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Selección del Mejor Modelo\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Selección del mejor modelo\n",
    "def select_best_model():\n",
    "    \"\"\"Selecciona el mejor modelo basado en criterios de negocio.\"\"\"\n",
    "    \n",
    "    print(\"Seleccionando mejor modelo...\")\n",
    "    \n",
    "    # Criterios de selección ponderados\n",
    "    print(\"Criterios de selección:\")\n",
    "    print(\"  1. AUC-ROC (40%) - Capacidad discriminativa\")\n",
    "    print(\"  2. Precision@10% (30%) - Relevante para campañas\")\n",
    "    print(\"  3. Recall (20%) - Captura de casos reales\")\n",
    "    print(\"  4. Precision (10%) - Reducción de falsos positivos\")\n",
    "    \n",
    "    # Calcular score ponderado\n",
    "    comparison_df_scored = comparison_df.copy()\n",
    "    comparison_df_scored['Score_Ponderado'] = (\n",
    "        comparison_df_scored['AUC_ROC'] * 0.4 +\n",
    "        comparison_df_scored['Precision@10%'] * 0.3 +\n",
    "        comparison_df_scored['Recall'] * 0.2 +\n",
    "        comparison_df_scored['Precision'] * 0.1\n",
    "    )\n",
    "    \n",
    "    # Encontrar mejor modelo\n",
    "    best_idx = comparison_df_scored['Score_Ponderado'].idxmax()\n",
    "    best_model_info = comparison_df_scored.iloc[best_idx]\n",
    "    best_model_result = all_results[best_idx]\n",
    "    \n",
    "    print(f\"\\nMejor modelo seleccionado:\")\n",
    "    print(f\"   Algoritmo: {best_model_info['Modelo']}\")\n",
    "    print(f\"   Estrategia: {best_model_info['Estrategia']}\")\n",
    "    print(f\"   Score Ponderado: {best_model_info['Score_Ponderado']:.3f}\")\n",
    "    print(f\"\\n   Métricas:\")\n",
    "    print(f\"     AUC-ROC: {best_model_info['AUC_ROC']:.3f}\")\n",
    "    print(f\"     Precision: {best_model_info['Precision']:.3f}\")\n",
    "    print(f\"     Recall: {best_model_info['Recall']:.3f}\")\n",
    "    print(f\"     F1-Score: {best_model_info['F1_Score']:.3f}\")\n",
    "    print(f\"     Precision@10%: {best_model_info['Precision@10%']:.3f}\")\n",
    "    \n",
    "    return best_model_result, best_model_info\n",
    "\n",
    "best_model, best_model_info = select_best_model()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Feature Importance y Validación\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Feature importance del mejor modelo\n",
    "def analyze_feature_importance():\n",
    "    \"\"\"Analiza la importancia de features del mejor modelo.\"\"\"\n",
    "    \n",
    "    print(f\"Analizando feature importance...\")\n",
    "    \n",
    "    model = best_model['model']\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Obtener feature names\n",
    "        feature_names = X_encoded.columns.tolist()\n",
    "        \n",
    "        # Crear DataFrame de importancia\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_,\n",
    "            'importance_pct': model.feature_importances_ * 100\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 variables más importantes:\")\n",
    "        for i, row in importance_df.head(10).iterrows():\n",
    "            print(f\"   {i+1:2d}. {row['feature']:<25} {row['importance_pct']:>6.2f}%\")\n",
    "        \n",
    "        # Visualización de feature importance\n",
    "        top_features = importance_df.head(15)\n",
    "        \n",
    "        fig_importance = px.bar(\n",
    "            top_features,\n",
    "            x='importance_pct',\n",
    "            y='feature',\n",
    "            orientation='h',\n",
    "            title='Top 15 Variables Más Importantes',\n",
    "            labels={'importance_pct': 'Importancia (%)', 'feature': 'Variables'},\n",
    "            color='importance_pct',\n",
    "            color_continuous_scale='Viridis'\n",
    "        )\n",
    "        \n",
    "        fig_importance.update_layout(\n",
    "            height=600,\n",
    "            yaxis={'categoryorder': 'total ascending'}\n",
    "        )\n",
    "        \n",
    "        fig_importance.show()\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"   El modelo no soporta feature importance\")\n",
    "        return None\n",
    "\n",
    "feature_importance_df = analyze_feature_importance()\n",
    "\n",
    "# %%\n",
    "# Validación cruzada del mejor modelo\n",
    "def cross_validation_analysis():\n",
    "    \"\"\"Realiza validación cruzada del mejor modelo.\"\"\"\n",
    "    \n",
    "    print(\"Ejecutando validación cruzada...\")\n",
    "    \n",
    "    # Preparar datos según la estrategia del mejor modelo\n",
    "    best_strategy = best_model['strategy']\n",
    "    strategy_config = balancing_strategies[best_strategy]\n",
    "    \n",
    "    print(f\"Validando: {best_model['model_name']} + {best_strategy}\")\n",
    "    \n",
    "    # Configurar modelo\n",
    "    if best_model['model_name'] == 'Random Forest':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        model = LogisticRegression(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            solver='liblinear'\n",
    "        )\n",
    "    \n",
    "    # Aplicar class weights si corresponde\n",
    "    if strategy_config['class_weight'] is not None:\n",
    "        if hasattr(model, 'class_weight'):\n",
    "            model.set_params(class_weight=strategy_config['class_weight'])\n",
    "    \n",
    "    # Preparar datos para CV\n",
    "    X_cv = strategy_config['X_train']\n",
    "    y_cv = strategy_config['y_train']\n",
    "    \n",
    "    # Configurar CV estratificado\n",
    "    cv_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Métricas a evaluar\n",
    "    scoring_metrics = ['roc_auc', 'precision', 'recall', 'f1']\n",
    "    cv_results = {}\n",
    "    \n",
    "    print(f\"Ejecutando {cv_folds}-fold cross-validation...\")\n",
    "    \n",
    "    for metric in scoring_metrics:\n",
    "        scores = cross_val_score(model, X_cv, y_cv, cv=skf, scoring=metric)\n",
    "        cv_results[metric] = {\n",
    "            'scores': scores,\n",
    "            'mean': scores.mean(),\n",
    "            'std': scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"{metric}: {scores.mean():.3f} (+/- {scores.std()*2:.3f})\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "cv_results = cross_validation_analysis()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 8. Predicciones Finales\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Generar predicciones finales\n",
    "def generate_final_predictions():\n",
    "    \"\"\"Genera predicciones finales en el dataset de test.\"\"\"\n",
    "    \n",
    "    print(\"Generando predicciones finales...\")\n",
    "    \n",
    "    # Entrenar modelo final con todos los datos de train\n",
    "    best_strategy = best_model['strategy']\n",
    "    strategy_config = balancing_strategies[best_strategy]\n",
    "    \n",
    "    print(f\"Entrenando modelo final:\")\n",
    "    print(f\"   Algoritmo: {best_model['model_name']}\")\n",
    "    print(f\"   Estrategia: {best_strategy}\")\n",
    "    \n",
    "    # Configurar modelo final\n",
    "    if best_model['model_name'] == 'Random Forest':\n",
    "        final_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        final_model = LogisticRegression(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            solver='liblinear'\n",
    "        )\n",
    "    \n",
    "    # Aplicar class weights si corresponde\n",
    "    if strategy_config['class_weight'] is not None:\n",
    "        if hasattr(final_model, 'class_weight'):\n",
    "            final_model.set_params(class_weight=strategy_config['class_weight'])\n",
    "    \n",
    "    # Entrenar modelo final\n",
    "    final_model.fit(strategy_config['X_train'], strategy_config['y_train'])\n",
    "    \n",
    "    # Generar predicciones en test\n",
    "    test_predictions = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    test_predictions_binary = final_model.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"Predicciones generadas:\")\n",
    "    print(f\"   Dataset test: {len(test_predictions):,} clientes\")\n",
    "    print(f\"   Rango probabilidades: {test_predictions.min():.4f} - {test_predictions.max():.4f}\")\n",
    "    print(f\"   Predicciones positivas: {test_predictions_binary.sum():,} ({test_predictions_binary.sum()/len(test_predictions_binary):.1%})\")\n",
    "    \n",
    "    # Crear DataFrame de resultados\n",
    "    results_df = pd.DataFrame({\n",
    "        'id': test_ids if test_ids is not None else range(len(test_predictions)),\n",
    "        'churn_probability': test_predictions,\n",
    "        'churn_prediction': test_predictions_binary\n",
    "    })\n",
    "    \n",
    "    return final_model, results_df, test_predictions\n",
    "\n",
    "final_model, predictions_df, test_probabilities = generate_final_predictions()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 9. Exportación de Resultados\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Exportar modelo y resultados\n",
    "output_dir = Path(\"../data/outputs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Exportando resultados...\")\n",
    "\n",
    "# Guardar predicciones\n",
    "predictions_path = output_dir / \"model_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "print(f\"Predicciones guardadas en: {predictions_path}\")\n",
    "\n",
    "# Guardar comparación de modelos\n",
    "comparison_path = output_dir / \"model_comparison.csv\"\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"Comparación guardada en: {comparison_path}\")\n",
    "\n",
    "# Guardar feature importance si está disponible\n",
    "if feature_importance_df is not None:\n",
    "    importance_path = output_dir / \"feature_importance.csv\"\n",
    "    feature_importance_df.to_csv(importance_path, index=False)\n",
    "    print(f\"Feature importance guardada en: {importance_path}\")\n",
    "\n",
    "# Guardar modelo final\n",
    "import joblib\n",
    "model_path = output_dir / \"best_model.pkl\"\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"Modelo guardado en: {model_path}\")\n",
    "\n",
    "# Guardar scaler y encoders\n",
    "scaler_path = output_dir / \"scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "encoders_path = output_dir / \"encoders.pkl\"\n",
    "joblib.dump(encoders, encoders_path)\n",
    "\n",
    "print(\"Componentes de preprocessing guardados\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 10. Resumen Ejecutivo\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Generar resumen ejecutivo final\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN EJECUTIVO - ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nMODELO SELECCIONADO:\")\n",
    "print(f\"   Algoritmo: {best_model_info['Modelo']}\")\n",
    "print(f\"   Estrategia de balanceo: {best_model_info['Estrategia']}\")\n",
    "\n",
    "print(f\"\\nPERFORMANCE DEL MODELO:\")\n",
    "print(f\"   AUC-ROC: {best_model_info['AUC_ROC']:.3f}\")\n",
    "print(f\"   Precision: {best_model_info['Precision']:.3f}\")\n",
    "print(f\"   Recall: {best_model_info['Recall']:.3f}\")\n",
    "print(f\"   F1-Score: {best_model_info['F1_Score']:.3f}\")\n",
    "print(f\"   Precision@10%: {best_model_info['Precision@10%']:.3f}\")\n",
    "\n",
    "if cv_results:\n",
    "    print(f\"\\nVALIDACIÓN CRUZADA:\")\n",
    "    print(f\"   AUC-ROC CV: {cv_results['roc_auc']['mean']:.3f} (+/- {cv_results['roc_auc']['std']*2:.3f})\")\n",
    "    print(f\"   Precision CV: {cv_results['precision']['mean']:.3f} (+/- {cv_results['precision']['std']*2:.3f})\")\n",
    "\n",
    "print(f\"\\nPREDICCIONES GENERADAS:\")\n",
    "print(f\"   Total clientes test: {len(predictions_df):,}\")\n",
    "print(f\"   Clientes en riesgo predichos: {predictions_df['churn_prediction'].sum():,}\")\n",
    "print(f\"   Tasa de fuga predicha: {predictions_df['churn_prediction'].mean():.1%}\")\n",
    "\n",
    "print(f\"\\nMANEJO DEL DESBALANCE:\")\n",
    "print(f\"   Problema original: {imbalance_stats['imbalance_ratio']:.0f}:1\")\n",
    "print(f\"   Estrategia exitosa: {best_model['strategy']}\")\n",
    "\n",
    "if feature_importance_df is not None:\n",
    "    top_3_features = feature_importance_df.head(3)['feature'].tolist()\n",
    "    print(f\"\\nTOP 3 VARIABLES IMPORTANTES:\")\n",
    "    for i, feature in enumerate(top_3_features, 1):\n",
    "        importance = feature_importance_df[feature_importance_df['feature'] == feature]['importance_pct'].iloc[0]\n",
    "        print(f\"   {i}. {feature}: {importance:.1f}%\")\n",
    "\n",
    "print(f\"\\nVALOR PARA COLSUBSIDIO:\")\n",
    "print(f\"   - Modelo robusto validado con múltiples estrategias\")\n",
    "print(f\"   - Capacidad de identificar clientes en riesgo\")\n",
    "print(f\"   - Base para campañas de retención focalizadas\")\n",
    "print(f\"   - Variables clave identificadas para monitoreo\")\n",
    "\n",
    "print(f\"\\nPRÓXIMOS PASOS:\")\n",
    "print(f\"   1. Segmentación de riesgo para campañas\")\n",
    "print(f\"   2. Cálculo de ROI y estrategias de retención\")\n",
    "print(f\"   3. Análisis de drivers de fuga\")\n",
    "print(f\"   4. Implementación en ambiente productivo\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"ENTRENAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "print(f\"Completado: {pd.Timestamp.now()}\")\n",
    "print(f\"Listo para Business Logic (Notebook 05)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
