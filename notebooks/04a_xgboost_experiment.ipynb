{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688283a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento XGBoost - Modelo de Fuga Colsubsidio\n",
    "# ================================================\n",
    "# \n",
    "# En este notebook vamos a probar XGBoost como alternativa a Random Forest\n",
    "# La idea es ver si podemos mejorar el recall manteniendo buen AUC\n",
    "# \n",
    "# Estructura:\n",
    "# 1. Carga de datos ya procesados\n",
    "# 2. Configuración de XGBoost con manejo de desbalance  \n",
    "# 3. Entrenamiento y comparación con RF\n",
    "# 4. Selección del mejor modelo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Librerías ML\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Cargando librerías...\")\n",
    "print(f\"Experimento iniciado: {pd.Timestamp.now()}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "\n",
    "# Carga de datos ya procesados\n",
    "data_dir = Path(\"../data/processed\")\n",
    "outputs_dir = Path(\"../data/outputs\")\n",
    "\n",
    "# Primero verificamos que tengamos los archivos necesarios\n",
    "train_path = data_dir / \"train_with_features.csv\"\n",
    "test_path = data_dir / \"test_with_features.csv\"\n",
    "\n",
    "if not train_path.exists() or not test_path.exists():\n",
    "    print(\"ERROR: No se encuentran los datos con features\")\n",
    "    print(\"Necesitas ejecutar los notebooks 02 y 03 primero\")\n",
    "    sys.exit()\n",
    "\n",
    "# Cargamos los datasets\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Datos cargados exitosamente:\")\n",
    "print(f\"Train: {len(train_data):,} registros x {len(train_data.columns)} columnas\")\n",
    "print(f\"Test: {len(test_data):,} registros x {len(test_data.columns)} columnas\")\n",
    "\n",
    "# Revisamos el desbalance del target\n",
    "if 'Target' in train_data.columns:\n",
    "    target_dist = train_data['Target'].value_counts()\n",
    "    imbalance_ratio = target_dist[0] / target_dist[1]\n",
    "    print(f\"\\nDesbalance encontrado: {imbalance_ratio:.1f}:1\")\n",
    "    print(f\"No Fuga: {target_dist[0]:,}, Fuga: {target_dist[1]:,}\")\n",
    "else:\n",
    "    print(\"PROBLEMA: No encontramos la variable Target\")\n",
    "    sys.exit()\n",
    "\n",
    "# Función para preparar los datos\n",
    "def prepare_data_for_experiment(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Esta función prepara los datos para el experimento.\n",
    "    Aplicamos el mismo preprocesamiento que en el notebook principal.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Preparando datos para el experimento...\")\n",
    "    \n",
    "    # Separamos features y target\n",
    "    exclude_vars = ['id', 'Target']\n",
    "    X = train_df.drop(exclude_vars, axis=1)\n",
    "    y = train_df['Target']\n",
    "    X_test = test_df.drop(['id'], axis=1, errors='ignore')\n",
    "    test_ids = test_df['id'] if 'id' in test_df.columns else range(len(test_df))\n",
    "    \n",
    "    # Codificamos variables categóricas\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        # Rellenamos valores faltantes antes de codificar\n",
    "        X[col] = X[col].astype(str).fillna('Unknown')\n",
    "        X[col] = encoder.fit_transform(X[col])\n",
    "        encoders[col] = encoder\n",
    "        \n",
    "        # Aplicamos el mismo encoding al test\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = X_test[col].astype(str).fillna('Unknown')\n",
    "            try:\n",
    "                X_test[col] = encoder.transform(X_test[col])\n",
    "            except ValueError:\n",
    "                # Si hay categorías nuevas en test, las marcamos con -1\n",
    "                X_test[col] = X_test[col].apply(\n",
    "                    lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1\n",
    "                )\n",
    "    \n",
    "    # Rellenamos valores faltantes con 0\n",
    "    X = X.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "    \n",
    "    # Creamos split train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Datos preparados:\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  X_val: {X_val.shape}\")\n",
    "    print(f\"  X_test: {X_test.shape}\")\n",
    "    print(f\"  Variables categóricas procesadas: {len(categorical_cols)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, test_ids, encoders\n",
    "\n",
    "# Ejecutamos la preparación\n",
    "X_train, X_val, X_test, y_train, y_val, test_ids, encoders = prepare_data_for_experiment(train_data, test_data)\n",
    "\n",
    "# Configuración del modelo XGBoost\n",
    "def setup_xgboost_model(y_train):\n",
    "    \"\"\"\n",
    "    Configuramos XGBoost con scale_pos_weight para manejar el desbalance.\n",
    "    Esta es una técnica más elegante que oversampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculamos el peso para balancear las clases\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    scale_pos_weight = neg_count / pos_count\n",
    "    \n",
    "    print(\"Configuración XGBoost:\")\n",
    "    print(f\"Ejemplos clase negativa: {neg_count:,}\")\n",
    "    print(f\"Ejemplos clase positiva: {pos_count:,}\")\n",
    "    print(f\"Scale pos weight calculado: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Creamos el modelo con parámetros optimizados para el caso\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,          # Suficiente para el dataset\n",
    "        max_depth=6,               # Evita overfitting\n",
    "        learning_rate=0.1,         # Learning rate conservador\n",
    "        subsample=0.8,             # Añade regularización\n",
    "        colsample_bytree=0.8,      # Feature sampling\n",
    "        scale_pos_weight=scale_pos_weight,  # Lo más importante para el desbalance\n",
    "        random_state=42,\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return xgb_model, scale_pos_weight\n",
    "\n",
    "xgb_model, scale_pos_weight = setup_xgboost_model(y_train)\n",
    "\n",
    "# Entrenamiento de XGBoost\n",
    "def train_xgboost_model():\n",
    "    \"\"\"Entrenamos XGBoost con early stopping para evitar overfitting.\"\"\"\n",
    "    \n",
    "    print(\"\\nEntrenando modelo XGBoost...\")\n",
    "    \n",
    "    # Entrenamos con validation set para early stopping\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False  # Sin output detallado\n",
    "    )\n",
    "    \n",
    "    # Hacemos predicciones en validation\n",
    "    y_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = xgb_model.predict(X_val)\n",
    "    \n",
    "    # Calculamos métricas\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    # Calculamos precision en top 10% (útil para campañas)\n",
    "    top_k = int(len(y_pred_proba) * 0.1)\n",
    "    top_indices = np.argsort(y_pred_proba)[-top_k:]\n",
    "    precision_at_k = y_val.iloc[top_indices].mean()\n",
    "    \n",
    "    print(\"Resultados XGBoost:\")\n",
    "    print(f\"  AUC-ROC: {auc:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "    print(f\"  Precision@10%: {precision_at_k:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': xgb_model,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'precision_at_k': precision_at_k,\n",
    "        'predictions_proba': y_pred_proba,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "xgb_results = train_xgboost_model()\n",
    "\n",
    "# Random Forest como baseline para comparar\n",
    "def train_random_forest_baseline():\n",
    "    \"\"\"Entrenamos Random Forest para tener un punto de comparación.\"\"\"\n",
    "    \n",
    "    print(\"\\nEntrenando Random Forest como baseline...\")\n",
    "    \n",
    "    # Usamos Random Forest con class weights para manejar el desbalance\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight='balanced',  # Esto maneja automáticamente el desbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Entrenamos el modelo\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones en validation\n",
    "    y_pred_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = rf_model.predict(X_val)\n",
    "    \n",
    "    # Calculamos las mismas métricas\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    # Precision en top 10%\n",
    "    top_k = int(len(y_pred_proba) * 0.1)\n",
    "    top_indices = np.argsort(y_pred_proba)[-top_k:]\n",
    "    precision_at_k = y_val.iloc[top_indices].mean()\n",
    "    \n",
    "    print(\"Resultados Random Forest:\")\n",
    "    print(f\"  AUC-ROC: {auc:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "    print(f\"  Precision@10%: {precision_at_k:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': rf_model,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'precision_at_k': precision_at_k,\n",
    "        'predictions_proba': y_pred_proba,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "rf_results = train_random_forest_baseline()\n",
    "\n",
    "# Comparación entre los dos modelos\n",
    "def compare_models():\n",
    "    \"\"\"Comparamos XGBoost vs Random Forest de forma detallada.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"COMPARACIÓN: XGBOOST vs RANDOM FOREST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Armamos una tabla comparativa\n",
    "    comparison_data = {\n",
    "        'Métrica': ['AUC-ROC', 'Precision', 'Recall', 'F1-Score', 'Precision@10%'],\n",
    "        'XGBoost': [\n",
    "            f\"{xgb_results['auc']:.3f}\",\n",
    "            f\"{xgb_results['precision']:.3f}\",\n",
    "            f\"{xgb_results['recall']:.3f}\",\n",
    "            f\"{xgb_results['f1']:.3f}\",\n",
    "            f\"{xgb_results['precision_at_k']:.3f}\"\n",
    "        ],\n",
    "        'Random Forest': [\n",
    "            f\"{rf_results['auc']:.3f}\",\n",
    "            f\"{rf_results['precision']:.3f}\",\n",
    "            f\"{rf_results['recall']:.3f}\",\n",
    "            f\"{rf_results['f1']:.3f}\",\n",
    "            f\"{rf_results['precision_at_k']:.3f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nTabla de comparación:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Calculamos las mejoras porcentuales\n",
    "    print(f\"\\nAnálisis de mejoras:\")\n",
    "    \n",
    "    auc_improvement = ((xgb_results['auc'] - rf_results['auc']) / rf_results['auc']) * 100\n",
    "    recall_improvement = ((xgb_results['recall'] - rf_results['recall']) / rf_results['recall']) * 100\n",
    "    precision_change = ((xgb_results['precision'] - rf_results['precision']) / rf_results['precision']) * 100\n",
    "    \n",
    "    print(f\"  AUC-ROC: {auc_improvement:+.1f}%\")\n",
    "    print(f\"  Recall: {recall_improvement:+.1f}%\")\n",
    "    print(f\"  Precision: {precision_change:+.1f}%\")\n",
    "    \n",
    "    # Determinamos cuál es mejor\n",
    "    if xgb_results['auc'] > rf_results['auc']:\n",
    "        winner = 'XGBoost'\n",
    "        print(f\"\\nGanador: {winner} (mejor AUC-ROC)\")\n",
    "    elif rf_results['auc'] > xgb_results['auc']:\n",
    "        winner = 'Random Forest'\n",
    "        print(f\"\\nGanador: {winner} (mejor AUC-ROC)\")\n",
    "    else:\n",
    "        winner = 'Empate'\n",
    "        print(f\"\\nResultado: Empate técnico\")\n",
    "    \n",
    "    return comparison_df, winner\n",
    "\n",
    "comparison_df, winner = compare_models()\n",
    "\n",
    "# Visualización de la comparación\n",
    "def visualize_comparison():\n",
    "    \"\"\"Creamos gráficos para comparar visualmente los modelos.\"\"\"\n",
    "    \n",
    "    # Datos para los gráficos\n",
    "    metrics = ['AUC-ROC', 'Precision', 'Recall', 'F1-Score']\n",
    "    xgb_values = [xgb_results['auc'], xgb_results['precision'], xgb_results['recall'], xgb_results['f1']]\n",
    "    rf_values = [rf_results['auc'], rf_results['precision'], rf_results['recall'], rf_results['f1']]\n",
    "    \n",
    "    # Gráfico de barras comparativo\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(name='XGBoost', x=metrics, y=xgb_values, marker_color='indianred'),\n",
    "        go.Bar(name='Random Forest', x=metrics, y=rf_values, marker_color='lightseagreen')\n",
    "    ])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Comparación de Performance: XGBoost vs Random Forest',\n",
    "        xaxis_title='Métricas',\n",
    "        yaxis_title='Score',\n",
    "        barmode='group',\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # También creamos un radar chart para ver el perfil completo\n",
    "    fig_radar = go.Figure()\n",
    "    \n",
    "    fig_radar.add_trace(go.Scatterpolar(\n",
    "        r=xgb_values + [xgb_values[0]],\n",
    "        theta=metrics + [metrics[0]],\n",
    "        fill='toself',\n",
    "        name='XGBoost',\n",
    "        line_color='indianred'\n",
    "    ))\n",
    "    \n",
    "    fig_radar.add_trace(go.Scatterpolar(\n",
    "        r=rf_values + [rf_values[0]],\n",
    "        theta=metrics + [metrics[0]],\n",
    "        fill='toself',\n",
    "        name='Random Forest',\n",
    "        line_color='lightseagreen'\n",
    "    ))\n",
    "    \n",
    "    fig_radar.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )),\n",
    "        title=\"Perfil de Performance: XGBoost vs Random Forest\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_radar.show()\n",
    "\n",
    "visualize_comparison() XGBOOST vs RANDOM FOREST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Crear tabla de comparación\n",
    "    comparison_data = {\n",
    "        'Métrica': ['AUC-ROC', 'Precision', 'Recall', 'F1-Score', 'Precision@10%'],\n",
    "        'XGBoost': [\n",
    "            f\"{xgb_results['auc']:.3f}\",\n",
    "            f\"{xgb_results['precision']:.3f}\",\n",
    "            f\"{xgb_results['recall']:.3f}\",\n",
    "            f\"{xgb_results['f1']:.3f}\",\n",
    "            f\"{xgb_results['precision_at_k']:.3f}\"\n",
    "        ],\n",
    "        'Random Forest': [\n",
    "            f\"{rf_results['auc']:.3f}\",\n",
    "            f\"{rf_results['precision']:.3f}\",\n",
    "            f\"{rf_results['recall']:.3f}\",\n",
    "            f\"{rf_results['f1']:.3f}\",\n",
    "            f\"{rf_results['precision_at_k']:.3f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n📊 TABLA DE COMPARACIÓN:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Calcular mejoras\n",
    "    print(f\"\\n🔍 ANÁLISIS DE MEJORAS:\")\n",
    "    \n",
    "    auc_improvement = ((xgb_results['auc'] - rf_results['auc']) / rf_results['auc']) * 100\n",
    "    recall_improvement = ((xgb_results['recall'] - rf_results['recall']) / rf_results['recall']) * 100\n",
    "    precision_change = ((xgb_results['precision'] - rf_results['precision']) / rf_results['precision']) * 100\n",
    "    \n",
    "    print(f\"  AUC-ROC: {auc_improvement:+.1f}% {'📈' if auc_improvement > 0 else '📉'}\")\n",
    "    print(f\"  Recall: {recall_improvement:+.1f}% {'📈' if recall_improvement > 0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
