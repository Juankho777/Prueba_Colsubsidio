{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f75819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Modelo de Fuga Colsubsidio\n",
    "# ====================================================\n",
    "# \n",
    "# Objetivo: Crear variables derivadas con l√≥gica de negocio crediticio\n",
    "# - Variables de utilizaci√≥n y comportamiento financiero\n",
    "# - √çndices de stress y actividad del cliente\n",
    "# - Variables espec√≠ficas de beneficios Colsubsidio\n",
    "# - Validaci√≥n de calidad de features creadas\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Configuraci√≥n Inicial y Carga de Datos Preparados\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Configuraci√≥n inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Importar m√≥dulos del proyecto\n",
    "sys.path.append('..')\n",
    "from src.feature_engineering import FeatureEngineer\n",
    "from src.preprocessing import DataPreprocessor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Librer√≠as y m√≥dulos cargados exitosamente\")\n",
    "print(f\"Feature Engineering iniciado: {pd.Timestamp.now()}\")\n",
    "\n",
    "# %%\n",
    "# Cargar datos preparados del notebook anterior\n",
    "data_dir = Path(\"../data/processed\")\n",
    "\n",
    "# Verificar que existen los archivos\n",
    "train_path = data_dir / \"train_cleaned_integrated.csv\"\n",
    "test_path = data_dir / \"test_cleaned_integrated.csv\"\n",
    "\n",
    "if not train_path.exists() or not test_path.exists():\n",
    "    print(\"‚ö†Ô∏è ALERTA: Archivos de datos preparados no encontrados\")\n",
    "    print(\"Ejecutar primero notebook 02_data_preparation.ipynb\")\n",
    "    sys.exit()\n",
    "\n",
    "# Cargar datasets\n",
    "train_clean = pd.read_csv(train_path)\n",
    "test_clean = pd.read_csv(test_path)\n",
    "\n",
    "print(\"=== DATOS CARGADOS ===\")\n",
    "print(f\"Train: {len(train_clean):,} registros x {len(train_clean.columns)} columnas\")\n",
    "print(f\"Test: {len(test_clean):,} registros x {len(test_clean.columns)} columnas\")\n",
    "\n",
    "# Verificar target\n",
    "if 'Target' in train_clean.columns:\n",
    "    target_dist = train_clean['Target'].value_counts()\n",
    "    print(f\"Distribuci√≥n Target: {target_dist.to_dict()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Variable Target no encontrada\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. An√°lisis Pre-Feature Engineering\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# An√°lisis de variables base disponibles\n",
    "def analyze_base_variables():\n",
    "    \"\"\"Analiza las variables disponibles para feature engineering.\"\"\"\n",
    "    \n",
    "    print(\"=== AN√ÅLISIS DE VARIABLES BASE ===\")\n",
    "    \n",
    "    # Categorizar variables disponibles\n",
    "    financial_vars = []\n",
    "    demographic_vars = []\n",
    "    benefit_vars = []\n",
    "    other_vars = []\n",
    "    \n",
    "    # Variables financieras esperadas\n",
    "    expected_financial = [\n",
    "        'Saldo', 'Limite.Cupo', 'Edad.Mora', 'Vr.Mora', \n",
    "        'Pagos.Mes.Ant', 'Vtas.Mes.Ant', 'Saldos.Mes.Ant',\n",
    "        'Disponible.Avances', 'Limite.Avances', 'Total.Intereses'\n",
    "    ]\n",
    "    \n",
    "    # Variables demogr√°ficas esperadas\n",
    "    expected_demographic = [\n",
    "        'edad', 'segmento', 'estrato', 'Genero', \n",
    "        'nivel_educativo', 'estado_civil', 'contrato'\n",
    "    ]\n",
    "    \n",
    "    # Variables de beneficios esperadas\n",
    "    expected_benefits = ['cuota_monetaria', 'sub_vivenda', 'bono_lonchera']\n",
    "    \n",
    "    # Clasificar variables disponibles\n",
    "    for col in train_clean.columns:\n",
    "        if col in expected_financial:\n",
    "            financial_vars.append(col)\n",
    "        elif col in expected_demographic:\n",
    "            demographic_vars.append(col)\n",
    "        elif col in expected_benefits:\n",
    "            benefit_vars.append(col)\n",
    "        elif col not in ['id', 'Target']:\n",
    "            other_vars.append(col)\n",
    "    \n",
    "    print(f\"\\nüìä INVENTARIO DE VARIABLES:\")\n",
    "    print(f\"  üí∞ Financieras: {len(financial_vars)} variables\")\n",
    "    print(f\"      {financial_vars[:5]}{'...' if len(financial_vars) > 5 else ''}\")\n",
    "    print(f\"  üë• Demogr√°ficas: {len(demographic_vars)} variables\")\n",
    "    print(f\"      {demographic_vars[:5]}{'...' if len(demographic_vars) > 5 else ''}\")\n",
    "    print(f\"  üèõÔ∏è Beneficios: {len(benefit_vars)} variables\")\n",
    "    print(f\"      {benefit_vars}\")\n",
    "    print(f\"  üìã Otras: {len(other_vars)} variables\")\n",
    "    \n",
    "    return {\n",
    "        'financial': financial_vars,\n",
    "        'demographic': demographic_vars,\n",
    "        'benefits': benefit_vars,\n",
    "        'other': other_vars\n",
    "    }\n",
    "\n",
    "variable_inventory = analyze_base_variables()\n",
    "\n",
    "# %%\n",
    "# Estad√≠sticas descriptivas de variables clave\n",
    "def descriptive_statistics():\n",
    "    \"\"\"Genera estad√≠sticas descriptivas de variables clave.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== ESTAD√çSTICAS DESCRIPTIVAS PRE-FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    # Variables financieras clave\n",
    "    key_financial = ['Saldo', 'Limite.Cupo', 'Edad.Mora', 'Pagos.Mes.Ant']\n",
    "    available_financial = [var for var in key_financial if var in train_clean.columns]\n",
    "    \n",
    "    if available_financial:\n",
    "        print(f\"\\nüí∞ VARIABLES FINANCIERAS:\")\n",
    "        for var in available_financial:\n",
    "            stats = train_clean[var].describe()\n",
    "            nulls = train_clean[var].isnull().sum()\n",
    "            zeros = (train_clean[var] == 0).sum()\n",
    "            \n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"  Count: {stats['count']:,.0f} (Nulls: {nulls:,}, Zeros: {zeros:,})\")\n",
    "            print(f\"  Mean: ${stats['mean']:,.0f}\")\n",
    "            print(f\"  Median: ${stats['50%']:,.0f}\")\n",
    "            print(f\"  Std: ${stats['std']:,.0f}\")\n",
    "            print(f\"  Range: ${stats['min']:,.0f} - ${stats['max']:,.0f}\")\n",
    "            \n",
    "            # Detectar outliers\n",
    "            q99 = train_clean[var].quantile(0.99)\n",
    "            outliers = (train_clean[var] > q99).sum()\n",
    "            if outliers > 0:\n",
    "                print(f\"  ‚ö†Ô∏è Outliers (>P99): {outliers:,} ({outliers/len(train_clean)*100:.1f}%)\")\n",
    "\n",
    "descriptive_statistics()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Creaci√≥n de Variables Derivadas\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Inicializar Feature Engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "print(\"=== INICIANDO FEATURE ENGINEERING ===\")\n",
    "print(\"Aplicando transformaciones con l√≥gica de negocio crediticio...\")\n",
    "\n",
    "# Aplicar feature engineering a train\n",
    "print(\"\\nüîß PROCESANDO DATASET TRAIN...\")\n",
    "train_enhanced = feature_engineer.apply_all_transformations(train_clean.copy())\n",
    "\n",
    "# Aplicar feature engineering a test\n",
    "print(\"\\nüîß PROCESANDO DATASET TEST...\")\n",
    "test_enhanced = feature_engineer.apply_all_transformations(test_clean.copy())\n",
    "\n",
    "print(f\"\\n‚úÖ FEATURE ENGINEERING COMPLETADO\")\n",
    "print(f\"Train: {len(train_clean.columns)} ‚Üí {len(train_enhanced.columns)} columnas (+{len(train_enhanced.columns) - len(train_clean.columns)})\")\n",
    "print(f\"Test: {len(test_clean.columns)} ‚Üí {len(test_enhanced.columns)} columnas (+{len(test_enhanced.columns) - len(test_clean.columns)})\")\n",
    "\n",
    "# %%\n",
    "# Analizar nuevas variables creadas\n",
    "def analyze_new_features():\n",
    "    \"\"\"Analiza las variables derivadas creadas.\"\"\"\n",
    "    \n",
    "    original_cols = set(train_clean.columns)\n",
    "    enhanced_cols = set(train_enhanced.columns)\n",
    "    new_features = enhanced_cols - original_cols\n",
    "    \n",
    "    print(f\"\\n=== NUEVAS VARIABLES CREADAS ===\")\n",
    "    print(f\"Total nuevas variables: {len(new_features)}\")\n",
    "    \n",
    "    for feature in sorted(new_features):\n",
    "        print(f\"\\nüìä {feature.upper()}:\")\n",
    "        \n",
    "        # Estad√≠sticas b√°sicas\n",
    "        feature_data = train_enhanced[feature]\n",
    "        \n",
    "        if feature_data.dtype in ['object', 'category']:\n",
    "            # Variable categ√≥rica\n",
    "            value_counts = feature_data.value_counts()\n",
    "            print(f\"  Tipo: Categ√≥rica\")\n",
    "            print(f\"  Valores √∫nicos: {feature_data.nunique()}\")\n",
    "            print(f\"  Distribuci√≥n:\")\n",
    "            for value, count in value_counts.head(5).items():\n",
    "                pct = count / len(feature_data) * 100\n",
    "                print(f\"    {value}: {count:,} ({pct:.1f}%)\")\n",
    "        else:\n",
    "            # Variable num√©rica\n",
    "            stats = feature_data.describe()\n",
    "            nulls = feature_data.isnull().sum()\n",
    "            zeros = (feature_data == 0).sum()\n",
    "            \n",
    "            print(f\"  Tipo: Num√©rica\")\n",
    "            print(f\"  Range: {stats['min']:.3f} - {stats['max']:.3f}\")\n",
    "            print(f\"  Mean: {stats['mean']:.3f}\")\n",
    "            print(f\"  Median: {stats['50%']:.3f}\")\n",
    "            print(f\"  Nulls: {nulls:,}, Zeros: {zeros:,}\")\n",
    "            \n",
    "            # Verificar distribuci√≥n por target si es num√©rica\n",
    "            if 'Target' in train_enhanced.columns and feature_data.nunique() > 1:\n",
    "                target_stats = train_enhanced.groupby('Target')[feature].agg(['mean', 'median']).round(3)\n",
    "                if len(target_stats) >= 2:\n",
    "                    print(f\"  Por Target:\")\n",
    "                    print(f\"    No Fuga: Mean={target_stats.loc[0, 'mean']}, Median={target_stats.loc[0, 'median']}\")\n",
    "                    print(f\"    Fuga: Mean={target_stats.loc[1, 'mean']}, Median={target_stats.loc[1, 'median']}\")\n",
    "\n",
    "analyze_new_features()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Visualizaci√≥n de Variables Derivadas Clave\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Visualizaci√≥n de utilization_ratio\n",
    "if 'utilization_ratio' in train_enhanced.columns:\n",
    "    \n",
    "    print(\"\\n=== VISUALIZACI√ìN: UTILIZATION RATIO ===\")\n",
    "    \n",
    "    # An√°lisis por target\n",
    "    fig_util = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=['Distribuci√≥n por Target', 'Boxplot por Target'],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Histograma por target\n",
    "    for target, color, name in [(0, 'blue', 'No Fuga'), (1, 'red', 'Fuga')]:\n",
    "        data = train_enhanced[train_enhanced['Target'] == target]['utilization_ratio']\n",
    "        # Filtrar valores extremos para mejor visualizaci√≥n\n",
    "        data_filtered = data[data <= data.quantile(0.95)]\n",
    "        \n",
    "        fig_util.add_trace(\n",
    "            go.Histogram(\n",
    "                x=data_filtered,\n",
    "                name=name,\n",
    "                opacity=0.7,\n",
    "                marker_color=color,\n",
    "                nbinsx=50\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Boxplot por target\n",
    "    for target, name in [(0, 'No Fuga'), (1, 'Fuga')]:\n",
    "        data = train_enhanced[train_enhanced['Target'] == target]['utilization_ratio']\n",
    "        data_filtered = data[data <= data.quantile(0.95)]\n",
    "        \n",
    "        fig_util.add_trace(\n",
    "            go.Box(\n",
    "                y=data_filtered,\n",
    "                name=name,\n",
    "                boxpoints='outliers'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig_util.update_layout(\n",
    "        title_text=\"An√°lisis de Utilization Ratio por Target\",\n",
    "        title_font_size=16,\n",
    "        height=500,\n",
    "        barmode='overlay'\n",
    "    )\n",
    "    \n",
    "    fig_util.show()\n",
    "    \n",
    "    # Estad√≠sticas por target\n",
    "    util_stats = train_enhanced.groupby('Target')['utilization_ratio'].agg(['count', 'mean', 'median', 'std']).round(3)\n",
    "    print(\"\\nEstad√≠sticas Utilization Ratio por Target:\")\n",
    "    for target in [0, 1]:\n",
    "        if target in util_stats.index:\n",
    "            label = \"No Fuga\" if target == 0 else \"Fuga\"\n",
    "            print(f\"  {label}: Mean={util_stats.loc[target, 'mean']:.3f}, Median={util_stats.loc[target, 'median']:.3f}\")\n",
    "\n",
    "# %%\n",
    "# Visualizaci√≥n de financial_stress y client_activity\n",
    "if 'financial_stress' in train_enhanced.columns and 'client_activity' in train_enhanced.columns:\n",
    "    \n",
    "    print(\"\\n=== VISUALIZACI√ìN: STRESS FINANCIERO Y ACTIVIDAD ===\")\n",
    "    \n",
    "    # Crear crosstabs\n",
    "    stress_target = pd.crosstab(train_enhanced['financial_stress'], train_enhanced['Target'], normalize='columns') * 100\n",
    "    activity_target = pd.crosstab(train_enhanced['client_activity'], train_enhanced['Target'], normalize='columns') * 100\n",
    "    \n",
    "    # Gr√°fico combinado\n",
    "    fig_combined = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=['Financial Stress por Target (%)', 'Client Activity por Target (%)'],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Financial Stress\n",
    "    fig_combined.add_trace(\n",
    "        go.Bar(\n",
    "            x=stress_target.index,\n",
    "            y=stress_target[0] if 0 in stress_target.columns else [],\n",
    "            name='No Fuga',\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig_combined.add_trace(\n",
    "        go.Bar(\n",
    "            x=stress_target.index,\n",
    "            y=stress_target[1] if 1 in stress_target.columns else [],\n",
    "            name='Fuga',\n",
    "            marker_color='lightcoral'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Client Activity\n",
    "    fig_combined.add_trace(\n",
    "        go.Bar(\n",
    "            x=activity_target.index,\n",
    "            y=activity_target[0] if 0 in activity_target.columns else [],\n",
    "            name='No Fuga',\n",
    "            marker_color='lightblue',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig_combined.add_trace(\n",
    "        go.Bar(\n",
    "            x=activity_target.index,\n",
    "            y=activity_target[1] if 1 in activity_target.columns else [],\n",
    "            name='Fuga',\n",
    "            marker_color='lightcoral',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig_combined.update_layout(\n",
    "        title_text=\"An√°lisis de Variables de Comportamiento\",\n",
    "        title_font_size=16,\n",
    "        height=500,\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    fig_combined.show()\n",
    "\n",
    "# %%\n",
    "# Heatmap de correlaci√≥n de nuevas variables\n",
    "def correlation_heatmap_new_features():\n",
    "    \"\"\"Crear heatmap de correlaci√≥n incluyendo nuevas variables.\"\"\"\n",
    "    \n",
    "    # Variables derivadas num√©ricas\n",
    "    derived_numeric = []\n",
    "    original_cols = set(train_clean.columns)\n",
    "    \n",
    "    for col in train_enhanced.columns:\n",
    "        if col not in original_cols and train_enhanced[col].dtype in ['int64', 'float64']:\n",
    "            derived_numeric.append(col)\n",
    "    \n",
    "    # Agregar algunas variables originales clave\n",
    "    key_original = ['Saldo', 'Limite.Cupo', 'Edad.Mora', 'Target']\n",
    "    available_original = [col for col in key_original if col in train_enhanced.columns]\n",
    "    \n",
    "    # Combinar variables para correlaci√≥n\n",
    "    correlation_vars = derived_numeric + available_original\n",
    "    correlation_vars = [col for col in correlation_vars if col in train_enhanced.columns]\n",
    "    \n",
    "    if len(correlation_vars) > 3:\n",
    "        print(f\"\\n=== MATRIZ DE CORRELACI√ìN: NUEVAS VARIABLES ===\")\n",
    "        print(f\"Variables incluidas: {correlation_vars}\")\n",
    "        \n",
    "        # Calcular correlaci√≥n\n",
    "        corr_matrix = train_enhanced[correlation_vars].corr()\n",
    "        \n",
    "        # Crear heatmap\n",
    "        fig_corr = px.imshow(\n",
    "            corr_matrix,\n",
    "            title=\"Correlaci√≥n: Variables Derivadas + Variables Clave\",\n",
    "            color_continuous_scale='RdBu',\n",
    "            aspect='auto',\n",
    "            text_auto=True\n",
    "        )\n",
    "        \n",
    "        fig_corr.update_layout(\n",
    "            title_font_size=16,\n",
    "            height=600,\n",
    "            width=800\n",
    "        )\n",
    "        \n",
    "        fig_corr.show()\n",
    "        \n",
    "        # Mostrar correlaciones m√°s altas con Target\n",
    "        if 'Target' in corr_matrix.columns:\n",
    "            target_corr = corr_matrix['Target'].drop('Target').sort_values(key=abs, ascending=False)\n",
    "            print(f\"\\nTop correlaciones con Target:\")\n",
    "            for var, corr in target_corr.head(5).items():\n",
    "                direction = \"positiva\" if corr > 0 else \"negativa\"\n",
    "                print(f\"  {var}: {corr:.3f} (correlaci√≥n {direction})\")\n",
    "\n",
    "correlation_heatmap_new_features()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Validaci√≥n de Calidad de Features\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Validaci√≥n comprensiva de features\n",
    "validation_report = feature_engineer.validate_features(train_enhanced)\n",
    "\n",
    "print(\"=== VALIDACI√ìN DE CALIDAD DE FEATURES ===\")\n",
    "\n",
    "# Mostrar reporte de validaci√≥n\n",
    "print(f\"\\nTotal features despu√©s de engineering: {validation_report['total_features']}\")\n",
    "\n",
    "# Valores faltantes\n",
    "missing_features = {k: v for k, v in validation_report['missing_values'].items() if v > 0}\n",
    "if missing_features:\n",
    "    print(f\"\\n‚ö†Ô∏è Features con valores faltantes:\")\n",
    "    for feature, count in list(missing_features.items())[:5]:\n",
    "        print(f\"  {feature}: {count:,} valores\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No hay valores faltantes en features derivadas\")\n",
    "\n",
    "# Valores infinitos\n",
    "infinite_features = validation_report['infinite_values']\n",
    "if infinite_features:\n",
    "    print(f\"\\n‚ö†Ô∏è Features con valores infinitos:\")\n",
    "    for feature, count in infinite_features.items():\n",
    "        print(f\"  {feature}: {count:,} valores infinitos\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No hay valores infinitos detectados\")\n",
    "\n",
    "# Valores negativos en features que deber√≠an ser positivas\n",
    "negative_features = validation_report['negative_values']\n",
    "if negative_features:\n",
    "    print(f\"\\n‚ö†Ô∏è Features con valores negativos inesperados:\")\n",
    "    for feature, count in negative_features.items():\n",
    "        print(f\"  {feature}: {count:,} valores negativos\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Rangos de valores consistentes con l√≥gica de negocio\")\n",
    "\n",
    "# Rangos de features clave\n",
    "feature_ranges = validation_report['feature_ranges']\n",
    "if feature_ranges:\n",
    "    print(f\"\\nüìä RANGOS DE FEATURES CLAVE:\")\n",
    "    for feature, stats in feature_ranges.items():\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Min: {stats['min']:.3f}\")\n",
    "        print(f\"  Max: {stats['max']:.3f}\")\n",
    "        print(f\"  Mean: {stats['mean']:.3f}\")\n",
    "        print(f\"  Std: {stats['std']:.3f}\")\n",
    "\n",
    "# %%\n",
    "# An√°lisis de poder discriminante de nuevas variables\n",
    "def discriminant_power_analysis():\n",
    "    \"\"\"Analiza el poder discriminante de las nuevas variables.\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== AN√ÅLISIS DE PODER DISCRIMINANTE ===\")\n",
    "    \n",
    "    if 'Target' not in train_enhanced.columns:\n",
    "        print(\"Target no disponible para an√°lisis discriminante\")\n",
    "        return\n",
    "    \n",
    "    # Variables derivadas para analizar\n",
    "    original_cols = set(train_clean.columns)\n",
    "    new_features = [col for col in train_enhanced.columns \n",
    "                   if col not in original_cols and train_enhanced[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    discriminant_results = []\n",
    "    \n",
    "    for feature in new_features:\n",
    "        # Calcular correlaci√≥n con target\n",
    "        corr_with_target = train_enhanced[feature].corr(train_enhanced['Target'])\n",
    "        \n",
    "        # Calcular diferencia de medias entre clases\n",
    "        group_means = train_enhanced.groupby('Target')[feature].mean()\n",
    "        if len(group_means) >= 2:\n",
    "            mean_diff = abs(group_means[1] - group_means[0])\n",
    "            mean_diff_pct = mean_diff / group_means[0] * 100 if group_means[0] != 0 else 0\n",
    "        else:\n",
    "            mean_diff = 0\n",
    "            mean_diff_pct = 0\n",
    "        \n",
    "        discriminant_results.append({\n",
    "            'feature': feature,\n",
    "            'correlation': abs(corr_with_target),\n",
    "            'mean_difference': mean_diff,\n",
    "            'mean_diff_pct': mean_diff_pct\n",
    "        })\n",
    "    \n",
    "    # Ordenar por correlaci√≥n\n",
    "    discriminant_df = pd.DataFrame(discriminant_results).sort_values('correlation', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTOP VARIABLES DERIVADAS POR PODER DISCRIMINANTE:\")\n",
    "    print(f\"{'Variable':<20} {'Correlaci√≥n':<12} {'Diff %':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for _, row in discriminant_df.head(8).iterrows():\n",
    "        print(f\"{row['feature']:<20} {row['correlation']:<12.3f} {row['mean_diff_pct']:<10.1f}\")\n",
    "    \n",
    "    return discriminant_df\n",
    "\n",
    "discriminant_analysis = discriminant_power_analysis()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Exportaci√≥n de Datos con Features\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Preparar y exportar datasets con features\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=== EXPORTACI√ìN DE DATOS CON FEATURES ===\")\n",
    "\n",
    "# Guardar train con features\n",
    "train_features_path = output_dir / \"train_with_features.csv\"\n",
    "train_enhanced.to_csv(train_features_path, index=False)\n",
    "print(f\"‚úÖ Train con features guardado: {train_features_path}\")\n",
    "print(f\"   {len(train_enhanced):,} registros x {len(train_enhanced.columns)} columnas\")\n",
    "\n",
    "# Guardar test con features\n",
    "test_features_path = output_dir / \"test_with_features.csv\"\n",
    "test_enhanced.to_csv(test_features_path, index=False)\n",
    "print(f\"‚úÖ Test con features guardado: {test_features_path}\")\n",
    "print(f\"   {len(test_enhanced):,} registros x {len(test_enhanced.columns)} columnas\")\n",
    "\n",
    "# Guardar reporte de feature engineering\n",
    "feature_report_path = output_dir / \"feature_engineering_report.txt\"\n",
    "with open(feature_report_path, 'w') as f:\n",
    "    f.write(\"REPORTE DE FEATURE ENGINEERING - COLSUBSIDIO CHURN MODEL\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Fecha de procesamiento: {pd.Timestamp.now()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"VARIABLES CREADAS:\\n\")\n",
    "    original_cols = set(train_clean.columns)\n",
    "    new_features = set(train_enhanced.columns) - original_cols\n",
    "    \n",
    "    for feature in sorted(new_features):\n",
    "        f.write(f\"- {feature}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nTOTAL NUEVAS VARIABLES: {len(new_features)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"L√ìGICA DE NEGOCIO APLICADA:\\n\")\n",
    "    f.write(\"1. utilization_ratio: Saldo / Limite.Cupo (utilizaci√≥n del cr√©dito)\\n\")\n",
    "    f.write(\"2. payment_behavior: Pagos.Mes.Ant / (Saldos.Mes.Ant + 1) (comportamiento de pago)\\n\")\n",
    "    f.write(\"3. financial_stress: Suma de indicadores de estr√©s financiero\\n\")\n",
    "    f.write(\"4. client_activity: Nivel de actividad transaccional\\n\")\n",
    "    f.write(\"5. benefits_index: Suma de beneficios Colsubsidio\\n\")\n",
    "    f.write(\"6. is_inactive: Flag de cliente completamente inactivo\\n\")\n",
    "    f.write(\"7. risk_profile: Categorizaci√≥n por d√≠as de mora\\n\")\n",
    "    f.write(\"8. util_category: Categorizaci√≥n de utilizaci√≥n de cupo\\n\")\n",
    "\n",
    "print(f\"‚úÖ Reporte guardado: {feature_report_path}\")\n",
    "\n",
    "# %%\n",
    "# Crear summary de variables por categor√≠a\n",
    "def create_feature_summary():\n",
    "    \"\"\"Crea resumen de variables por categor√≠a.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìã RESUMEN FINAL DE VARIABLES:\")\n",
    "    \n",
    "    # Obtener grupos de features\n",
    "    feature_groups = feature_engineer.get_feature_importance_groups()\n",
    "    \n",
    "    total_vars = 0\n",
    "    for group_name, variables in feature_groups.items():\n",
    "        available_vars = [var for var in variables if var in train_enhanced.columns]\n",
    "        total_vars += len(available_vars)\n",
    "        \n",
    "        print(f\"\\n{group_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Variables disponibles: {len(available_vars)}\")\n",
    "        if available_vars:\n",
    "            print(f\"  Ejemplos: {available_vars[:3]}{'...' if len(available_vars) > 3 else ''}\")\n",
    "    \n",
    "    # Variables nuevas derivadas\n",
    "    original_cols = set(train_clean.columns)\n",
    "    new_features = [col for col in train_enhanced.columns if col not in original_cols]\n",
    "    \n",
    "    print(f\"\\nVariables Derivadas Nuevas:\")\n",
    "    print(f\"  Total creadas: {len(new_features)}\")\n",
    "    print(f\"  Lista: {new_features}\")\n",
    "    \n",
    "    print(f\"\\nüìä ESTAD√çSTICAS FINALES:\")\n",
    "    print(f\"  Variables originales: {len(train_clean.columns)}\")\n",
    "    print(f\"  Variables nuevas: {len(new_features)}\")\n",
    "    print(f\"  Variables totales: {len(train_enhanced.columns)}\")\n",
    "    print(f\"  Incremento: {len(new_features)/len(train_clean.columns)*100:.1f}%\")\n",
    "\n",
    "create_feature_summary()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Resumen Ejecutivo de Feature Engineering\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Generar resumen ejecutivo completo\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN EJECUTIVO - FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# M√©tricas de transformaci√≥n\n",
    "original_features = len(train_clean.columns)\n",
    "enhanced_features = len(train_enhanced.columns)\n",
    "new_features_count = enhanced_features - original_features\n",
    "\n",
    "print(f\"\\nüîß TRANSFORMACI√ìN REALIZADA:\")\n",
    "print(f\"  Variables originales: {original_features}\")\n",
    "print(f\"  Variables nuevas creadas: {new_features_count}\")\n",
    "print(f\"  Variables totales: {enhanced_features}\")\n",
    "print(f\"  Incremento: {new_features_count/original_features*100:.1f}%\")\n",
    "\n",
    "# Estado de calidad\n",
    "final_missing = train_enhanced.isnull().sum().sum()\n",
    "infinite_count = sum(validation_report['infinite_values'].values()) if validation_report['infinite_values'] else 0\n",
    "\n",
    "print(f\"\\nüìã CALIDAD DE FEATURES:\")\n",
    "print(f\"  Valores faltantes: {final_missing:,}\")\n",
    "print(f\"  Valores infinitos: {infinite_count:,}\")\n",
    "print(f\"  Features num√©ricas: {len(train_enhanced.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"  Features categ√≥ricas: {len(train_enhanced.select_dtypes(include=['object']).columns)}\")\n",
    "\n",
    "# Variables m√°s discriminantes\n",
    "if 'discriminant_analysis' in locals() and len(discriminant_analysis) > 0:\n",
    "    top_discriminant = discriminant_analysis.head(3)\n",
    "    print(f\"\\nüéØ TOP VARIABLES DISCRIMINANTES:\")\n",
    "    for _, row in top_discriminant.iterrows():\n",
    "        print(f\"  {row['feature']}: correlaci√≥n {row['correlation']:.3f}\")\n",
    "\n",
    "# L√≥gica de negocio aplicada\n",
    "print(f\"\\nüíº L√ìGICA DE NEGOCIO APLICADA:\")\n",
    "business_features = [\n",
    "    \"‚úÖ Ratios financieros (utilizaci√≥n, comportamiento de pago)\",\n",
    "    \"‚úÖ √çndices de stress y actividad del cliente\", \n",
    "    \"‚úÖ Variables espec√≠ficas de beneficios Colsubsidio\",\n",
    "    \"‚úÖ Categorizaci√≥n de riesgo crediticio\",\n",
    "    \"‚úÖ Flags de comportamiento (inactividad)\"\n",
    "]\n",
    "\n",
    "for feature in business_features:\n",
    "    print(f\"  {feature}\")\n",
    "\n",
    "# Validaciones de calidad\n",
    "print(f\"\\nüîç VALIDACIONES REALIZADAS:\")\n",
    "validations = [\n",
    "    \"‚úÖ Rangos de valores consistentes con l√≥gica crediticia\",\n",
    "    \"‚úÖ No hay valores infinitos o NaN en variables derivadas\",\n",
    "    \"‚úÖ Correlaciones calculadas con variable target\",\n",
    "    \"‚úÖ Poder discriminante evaluado por variable\",\n",
    "    \"‚úÖ Distribuciones analizadas por target\"\n",
    "]\n",
    "\n",
    "for validation in validations:\n",
    "    print(f\"  {validation}\")\n",
    "\n",
    "# Impacto esperado\n",
    "print(f\"\\nüìà IMPACTO ESPERADO PARA EL MODELO:\")\n",
    "expected_impacts = [\n",
    "    \"üéØ Mayor capacidad predictiva con variables de comportamiento\",\n",
    "    \"üí∞ Variables financieras optimizadas para detecci√≥n de fuga\",\n",
    "    \"üèõÔ∏è Incorporaci√≥n de l√≥gica espec√≠fica de Colsubsidio\",\n",
    "    \"‚ö° Features engineered listos para algoritmos ML\",\n",
    "    \"üìä Base s√≥lida para segmentaci√≥n de riesgo\"\n",
    "]\n",
    "\n",
    "for impact in expected_impacts:\n",
    "    print(f\"  {impact}\")\n",
    "\n",
    "# Pr√≥ximos pasos\n",
    "print(f\"\\nüöÄ PR√ìXIMOS PASOS RECOMENDADOS:\")\n",
    "next_steps = [\n",
    "    \"1. ü§ñ MODELADO: Entrenar algoritmos con features optimizadas\",\n",
    "    \"2. üìä FEATURE SELECTION: Seleccionar variables m√°s importantes\",\n",
    "    \"3. ‚öñÔ∏è BALANCEO: Aplicar t√©cnicas para desbalance de clases\",\n",
    "    \"4. üî¨ VALIDACI√ìN: Cross-validation con features derivadas\",\n",
    "    \"5. üíº BUSINESS LOGIC: Traducir features a insights de negocio\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "# Alertas importantes\n",
    "print(f\"\\n‚ö†Ô∏è CONSIDERACIONES IMPORTANTES:\")\n",
    "considerations = []\n",
    "\n",
    "if final_missing > 0:\n",
    "    considerations.append(f\"Revisar {final_missing:,} valores faltantes restantes\")\n",
    "\n",
    "if infinite_count > 0:\n",
    "    considerations.append(f\"Corregir {infinite_count:,} valores infinitos detectados\")\n",
    "\n",
    "# Verificar correlaciones extremas\n",
    "if 'discriminant_analysis' in locals() and len(discriminant_analysis) > 0:\n",
    "    high_corr = discriminant_analysis[discriminant_analysis['correlation'] > 0.8]\n",
    "    if len(high_corr) > 0:\n",
    "        considerations.append(f\"Evaluar {len(high_corr)} variables con correlaci√≥n muy alta\")\n",
    "\n",
    "if len(considerations) > 0:\n",
    "    for consideration in considerations:\n",
    "        print(f\"  ‚ö†Ô∏è {consideration}\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ No se detectaron problemas cr√≠ticos\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
    "print(f\"üìÖ Completado: {pd.Timestamp.now()}\")\n",
    "print(f\"üéØ Features listos para Entrenamiento de Modelos (Notebook 04)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
