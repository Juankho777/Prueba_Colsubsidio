{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748dd835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:src.data_loader:Error cargando datasets: [Errno 2] No such file or directory: 'data\\\\raw\\\\train.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librer√≠as y m√≥dulos cargados exitosamente\n",
      "Preparaci√≥n iniciada: 2025-08-13 17:59:18.548814\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\raw\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Carga de datos usando m√≥dulo configurado\u001b[39;00m\n\u001b[0;32m     39\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../config/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m datasets \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mload_all_datasets()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Referencias directas para facilitar trabajo\u001b[39;00m\n\u001b[0;32m     43\u001b[0m train_raw \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\38509641\\colsubsidio-churn-model\\notebooks\\..\\src\\data_loader.py:111\u001b[0m, in \u001b[0;36mDataLoader.load_all_datasets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_train_data()\n\u001b[0;32m    112\u001b[0m     datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_test_data()\n\u001b[0;32m    113\u001b[0m     datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemograficas\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_demograficas_data()\n",
      "File \u001b[1;32mc:\\Users\\38509641\\colsubsidio-churn-model\\notebooks\\..\\src\\data_loader.py:56\u001b[0m, in \u001b[0;36mDataLoader.load_train_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Carga el dataset de entrenamiento.\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m file_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_names[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 56\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     57\u001b[0m     file_path,\n\u001b[0;32m     58\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseparator\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     59\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     62\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain cargado: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registros\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Limpiar variables financieras\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\raw\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "# Preparaci√≥n de Datos - Modelo de Fuga Colsubsidio\n",
    "# ====================================================\n",
    "# \n",
    "# Objetivo: Limpiar e integrar los datasets para crear una base s√≥lida para el modelado\n",
    "# - Limpieza de variables financieras con formato de texto\n",
    "# - Manejo inteligente de valores faltantes\n",
    "# - Integraci√≥n robusta de m√∫ltiples fuentes\n",
    "# - Validaci√≥n de calidad post-limpieza\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Configuraci√≥n Inicial y Carga de Datos\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Configuraci√≥n inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Importar m√≥dulos del proyecto\n",
    "sys.path.append('..')\n",
    "from src.data_loader import DataLoader\n",
    "from src.preprocessing import DataPreprocessor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Librer√≠as y m√≥dulos cargados exitosamente\")\n",
    "print(f\"Preparaci√≥n iniciada: {pd.Timestamp.now()}\")\n",
    "\n",
    "# %%\n",
    "# Carga de datos usando m√≥dulo configurado\n",
    "data_loader = DataLoader(config_path=\"../config/\")\n",
    "datasets = data_loader.load_all_datasets()\n",
    "\n",
    "# Referencias directas para facilitar trabajo\n",
    "train_raw = datasets['train'].copy()\n",
    "test_raw = datasets['test'].copy()\n",
    "demo_raw = datasets['demograficas'].copy()\n",
    "subs_raw = datasets['subsidios'].copy()\n",
    "\n",
    "print(\"=== DATASETS CARGADOS ===\")\n",
    "datasets_info = {\n",
    "    'Train': {'registros': len(train_raw), 'columnas': len(train_raw.columns)},\n",
    "    'Test': {'registros': len(test_raw), 'columnas': len(test_raw.columns)},\n",
    "    'Demogr√°ficas': {'registros': len(demo_raw), 'columnas': len(demo_raw.columns)},\n",
    "    'Subsidios': {'registros': len(subs_raw), 'columnas': len(subs_raw.columns)}\n",
    "}\n",
    "\n",
    "for name, info in datasets_info.items():\n",
    "    print(f\"{name}: {info['registros']:,} registros x {info['columnas']} columnas\")\n",
    "\n",
    "# Verificar estructura de columnas\n",
    "print(\"\\n=== VERIFICACI√ìN DE ESTRUCTURA ===\")\n",
    "print(f\"Columnas en Train: {list(train_raw.columns)[:5]}...\")  # Primeras 5\n",
    "print(f\"Columnas en Test: {list(test_raw.columns)[:5]}...\")   # Primeras 5\n",
    "\n",
    "# Verificar variable target\n",
    "has_target = 'Target' in train_raw.columns\n",
    "print(f\"\\nVariable Target presente: {'‚úÖ' if has_target else '‚ùå'}\")\n",
    "\n",
    "if has_target:\n",
    "    target_dist = train_raw['Target'].value_counts()\n",
    "    print(f\"Distribuci√≥n Target: {target_dist.to_dict()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Limpieza de Variables Financieras\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Identificar variables financieras que necesitan limpieza\n",
    "financial_columns = [\n",
    "    'Disponible.Avances', 'Limite.Avances', 'Total.Intereses',\n",
    "    'Saldos.Mes.Ant', 'Pagos.Mes.Ant', 'Vtas.Mes.Ant',\n",
    "    'Limite.Cupo', 'Pago.del.Mes', 'Pago.Minimo',\n",
    "    'Vr.Mora', 'Vr.Cuota.Manejo', 'Saldo'\n",
    "]\n",
    "\n",
    "# Filtrar columnas que realmente existen\n",
    "available_financial = [col for col in financial_columns if col in train_raw.columns]\n",
    "\n",
    "print(f\"Variables financieras a limpiar: {len(available_financial)}\")\n",
    "print(f\"Variables: {available_financial[:5]}...\")  # Mostrar primeras 5\n",
    "\n",
    "# %%\n",
    "# Funci√≥n de limpieza personalizada\n",
    "def clean_financial_variable(series, column_name):\n",
    "    \"\"\"Limpia una variable financiera espec√≠fica.\"\"\"\n",
    "    print(f\"\\nLimpiando {column_name}...\")\n",
    "    \n",
    "    # Estad√≠sticas antes de limpieza\n",
    "    original_type = series.dtype\n",
    "    original_nulls = series.isnull().sum()\n",
    "    \n",
    "    # Proceso de limpieza\n",
    "    cleaned = series.astype(str)\n",
    "    \n",
    "    # Remover caracteres comunes en formato monetario\n",
    "    replacements = [',', '$', ' ', '.00']\n",
    "    for char in replacements:\n",
    "        cleaned = cleaned.str.replace(char, '')\n",
    "    \n",
    "    # Manejar valores especiales\n",
    "    cleaned = cleaned.replace(['nan', 'NaN', 'None', ''], np.nan)\n",
    "    \n",
    "    # Convertir a num√©rico\n",
    "    cleaned_numeric = pd.to_numeric(cleaned, errors='coerce')\n",
    "    \n",
    "    # Estad√≠sticas despu√©s de limpieza\n",
    "    final_nulls = cleaned_numeric.isnull().sum()\n",
    "    new_nulls = final_nulls - original_nulls\n",
    "    \n",
    "    print(f\"  Tipo original: {original_type} ‚Üí float64\")\n",
    "    print(f\"  NaN originales: {original_nulls:,} ‚Üí finales: {final_nulls:,} (+{new_nulls:,})\")\n",
    "    \n",
    "    if len(cleaned_numeric.dropna()) > 0:\n",
    "        print(f\"  Rango: ${cleaned_numeric.min():,.0f} - ${cleaned_numeric.max():,.0f}\")\n",
    "    \n",
    "    return cleaned_numeric\n",
    "\n",
    "# Aplicar limpieza a train\n",
    "print(\"=== LIMPIEZA DATASET TRAIN ===\")\n",
    "train_clean = train_raw.copy()\n",
    "\n",
    "for col in available_financial:\n",
    "    if col in train_clean.columns:\n",
    "        train_clean[col] = clean_financial_variable(train_clean[col], col)\n",
    "\n",
    "# Aplicar limpieza a test\n",
    "print(\"\\n=== LIMPIEZA DATASET TEST ===\")\n",
    "test_clean = test_raw.copy()\n",
    "\n",
    "for col in available_financial:\n",
    "    if col in test_clean.columns:\n",
    "        test_clean[col] = clean_financial_variable(test_clean[col], col)\n",
    "\n",
    "print(\"\\n‚úÖ Limpieza de variables financieras completada\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. An√°lisis de Valores Faltantes Post-Limpieza\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# An√°lisis completo de valores faltantes\n",
    "def analyze_missing_values(df, dataset_name):\n",
    "    \"\"\"Analiza patrones de valores faltantes.\"\"\"\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_pct = (missing_data / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Variable': missing_data.index,\n",
    "        'Faltantes': missing_data.values,\n",
    "        'Porcentaje': missing_pct.values\n",
    "    }).query('Faltantes > 0').sort_values('Porcentaje', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== AN√ÅLISIS DE FALTANTES - {dataset_name.upper()} ===\")\n",
    "    print(f\"Variables con faltantes: {len(missing_df)} de {len(df.columns)}\")\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(\"\\nTop 10 variables con m√°s faltantes:\")\n",
    "        for _, row in missing_df.head(10).iterrows():\n",
    "            print(f\"  {row['Variable']}: {row['Faltantes']:,} ({row['Porcentaje']:.1f}%)\")\n",
    "        \n",
    "        # Categorizar por severidad\n",
    "        severe = missing_df[missing_df['Porcentaje'] > 90]\n",
    "        high = missing_df[(missing_df['Porcentaje'] > 50) & (missing_df['Porcentaje'] <= 90)]\n",
    "        medium = missing_df[(missing_df['Porcentaje'] > 10) & (missing_df['Porcentaje'] <= 50)]\n",
    "        low = missing_df[missing_df['Porcentaje'] <= 10]\n",
    "        \n",
    "        print(f\"\\nCategorizaci√≥n por severidad:\")\n",
    "        print(f\"  Severo (>90%): {len(severe)} variables\")\n",
    "        print(f\"  Alto (50-90%): {len(high)} variables\")\n",
    "        print(f\"  Medio (10-50%): {len(medium)} variables\")\n",
    "        print(f\"  Bajo (<10%): {len(low)} variables\")\n",
    "        \n",
    "        return missing_df\n",
    "    else:\n",
    "        print(\"‚úÖ No hay valores faltantes\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Analizar train y test\n",
    "missing_train = analyze_missing_values(train_clean, 'train')\n",
    "missing_test = analyze_missing_values(test_clean, 'test')\n",
    "\n",
    "# %%\n",
    "# Gr√°fico comparativo de faltantes\n",
    "if len(missing_train) > 0 or len(missing_test) > 0:\n",
    "    # Combinar datos para visualizaci√≥n\n",
    "    fig_missing = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=['Train Dataset', 'Test Dataset'],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    if len(missing_train) > 0:\n",
    "        top_missing_train = missing_train.head(10)\n",
    "        fig_missing.add_trace(\n",
    "            go.Bar(\n",
    "                y=top_missing_train['Variable'],\n",
    "                x=top_missing_train['Porcentaje'],\n",
    "                name='Train',\n",
    "                orientation='h',\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Test\n",
    "    if len(missing_test) > 0:\n",
    "        top_missing_test = missing_test.head(10)\n",
    "        fig_missing.add_trace(\n",
    "            go.Bar(\n",
    "                y=top_missing_test['Variable'],\n",
    "                x=top_missing_test['Porcentaje'],\n",
    "                name='Test',\n",
    "                orientation='h',\n",
    "                marker_color='lightcoral'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig_missing.update_layout(\n",
    "        title_text=\"Comparaci√≥n de Valores Faltantes - Train vs Test\",\n",
    "        title_font_size=16,\n",
    "        height=600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig_missing.update_xaxes(title_text=\"Porcentaje Faltante (%)\")\n",
    "    fig_missing.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Estrategia de Imputaci√≥n\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Estrategia de imputaci√≥n inteligente\n",
    "def apply_missing_strategy(df, dataset_name):\n",
    "    \"\"\"Aplica estrategia espec√≠fica seg√∫n el tipo de variable.\"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    print(f\"\\n=== ESTRATEGIA DE IMPUTACI√ìN - {dataset_name.upper()} ===\")\n",
    "    \n",
    "    # 1. Variables financieras: rellenar con 0 (sentido de negocio)\n",
    "    financial_vars = [col for col in available_financial if col in df_imputed.columns]\n",
    "    if financial_vars:\n",
    "        for col in financial_vars:\n",
    "            nulls_before = df_imputed[col].isnull().sum()\n",
    "            df_imputed[col] = df_imputed[col].fillna(0)\n",
    "            print(f\"  {col}: {nulls_before:,} NaN ‚Üí 0 (l√≥gica financiera)\")\n",
    "    \n",
    "    # 2. Variables categ√≥ricas: rellenar con 'Unknown'\n",
    "    categorical_vars = df_imputed.select_dtypes(include=['object']).columns\n",
    "    categorical_vars = [col for col in categorical_vars if col not in ['id']]\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        nulls_before = df_imputed[col].isnull().sum()\n",
    "        if nulls_before > 0:\n",
    "            df_imputed[col] = df_imputed[col].fillna('Unknown')\n",
    "            print(f\"  {col}: {nulls_before:,} NaN ‚Üí 'Unknown'\")\n",
    "    \n",
    "    # 3. Variables num√©ricas restantes: usar mediana\n",
    "    numeric_vars = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "    numeric_vars = [col for col in numeric_vars if col not in financial_vars + ['id', 'Target']]\n",
    "    \n",
    "    for col in numeric_vars:\n",
    "        nulls_before = df_imputed[col].isnull().sum()\n",
    "        if nulls_before > 0:\n",
    "            median_val = df_imputed[col].median()\n",
    "            df_imputed[col] = df_imputed[col].fillna(median_val)\n",
    "            print(f\"  {col}: {nulls_before:,} NaN ‚Üí {median_val:.1f} (mediana)\")\n",
    "    \n",
    "    # Verificar resultado\n",
    "    remaining_nulls = df_imputed.isnull().sum().sum()\n",
    "    print(f\"\\nValores faltantes restantes: {remaining_nulls:,}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Aplicar estrategia de imputaci√≥n\n",
    "train_imputed = apply_missing_strategy(train_clean, 'train')\n",
    "test_imputed = apply_missing_strategy(test_clean, 'test')\n",
    "\n",
    "print(\"\\n‚úÖ Imputaci√≥n de valores faltantes completada\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Integraci√≥n de Datasets\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Verificar consistencia de IDs antes del merge\n",
    "def verify_id_consistency():\n",
    "    \"\"\"Verifica que los IDs sean consistentes entre datasets.\"\"\"\n",
    "    \n",
    "    train_ids = set(train_imputed['id'].unique())\n",
    "    test_ids = set(test_imputed['id'].unique())\n",
    "    demo_ids = set(demo_raw['id'].unique())\n",
    "    subs_ids = set(subs_raw['id'].unique())\n",
    "    \n",
    "    print(\"=== VERIFICACI√ìN DE CONSISTENCIA DE IDs ===\")\n",
    "    print(f\"IDs √∫nicos Train: {len(train_ids):,}\")\n",
    "    print(f\"IDs √∫nicos Test: {len(test_ids):,}\")\n",
    "    print(f\"IDs √∫nicos Demogr√°ficas: {len(demo_ids):,}\")\n",
    "    print(f\"IDs √∫nicos Subsidios: {len(subs_ids):,}\")\n",
    "    \n",
    "    # Verificar solapamientos\n",
    "    all_main_ids = train_ids.union(test_ids)\n",
    "    demo_coverage = len(all_main_ids.intersection(demo_ids)) / len(all_main_ids)\n",
    "    subs_coverage = len(all_main_ids.intersection(subs_ids)) / len(all_main_ids)\n",
    "    \n",
    "    print(f\"\\nCobertura Demogr√°ficas: {demo_coverage:.1%}\")\n",
    "    print(f\"Cobertura Subsidios: {subs_coverage:.1%}\")\n",
    "    \n",
    "    # Verificar separaci√≥n train/test\n",
    "    overlap = train_ids.intersection(test_ids)\n",
    "    print(f\"\\nSolapamiento Train-Test: {len(overlap)} IDs\")\n",
    "    \n",
    "    if len(overlap) > 0:\n",
    "        print(\"‚ö†Ô∏è ALERTA: Hay solapamiento entre train y test\")\n",
    "    else:\n",
    "        print(\"‚úÖ Train y test est√°n correctamente separados\")\n",
    "    \n",
    "    return {\n",
    "        'demo_coverage': demo_coverage,\n",
    "        'subs_coverage': subs_coverage,\n",
    "        'train_test_overlap': len(overlap)\n",
    "    }\n",
    "\n",
    "id_verification = verify_id_consistency()\n",
    "\n",
    "# %%\n",
    "# Proceso de integraci√≥n con validaciones\n",
    "def integrate_datasets_safely():\n",
    "    \"\"\"Integra datasets con validaciones de calidad.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== PROCESO DE INTEGRACI√ìN ===\")\n",
    "    \n",
    "    # Merge train con demogr√°ficas\n",
    "    print(\"\\n1. Integrando TRAIN con DEMOGR√ÅFICAS...\")\n",
    "    train_demo = train_imputed.merge(\n",
    "        demo_raw, \n",
    "        on='id', \n",
    "        how='left', \n",
    "        suffixes=('', '_demo')\n",
    "    )\n",
    "    \n",
    "    print(f\"   Antes: {len(train_imputed):,} registros\")\n",
    "    print(f\"   Despu√©s: {len(train_demo):,} registros\")\n",
    "    print(f\"   Nuevas columnas: {len(train_demo.columns) - len(train_imputed.columns)}\")\n",
    "    \n",
    "    # Merge con subsidios\n",
    "    print(\"\\n2. Integrando con SUBSIDIOS...\")\n",
    "    train_integrated = train_demo.merge(\n",
    "        subs_raw,\n",
    "        on='id',\n",
    "        how='left',\n",
    "        suffixes=('', '_subs')\n",
    "    )\n",
    "    \n",
    "    print(f\"   Antes: {len(train_demo):,} registros\")\n",
    "    print(f\"   Despu√©s: {len(train_integrated):,} registros\")\n",
    "    print(f\"   Columnas finales: {len(train_integrated.columns)}\")\n",
    "    \n",
    "    # Mismo proceso para test\n",
    "    print(\"\\n3. Integrando TEST...\")\n",
    "    test_demo = test_imputed.merge(\n",
    "        demo_raw,\n",
    "        on='id',\n",
    "        how='left',\n",
    "        suffixes=('', '_demo')\n",
    "    )\n",
    "    \n",
    "    test_integrated = test_demo.merge(\n",
    "        subs_raw,\n",
    "        on='id', \n",
    "        how='left',\n",
    "        suffixes=('', '_subs')\n",
    "    )\n",
    "    \n",
    "    print(f\"   Test final: {len(test_integrated):,} registros x {len(test_integrated.columns)} columnas\")\n",
    "    \n",
    "    # Validaciones post-merge\n",
    "    print(\"\\n4. Validaciones post-integraci√≥n...\")\n",
    "    \n",
    "    # Verificar p√©rdida de registros\n",
    "    if len(train_integrated) != len(train_imputed):\n",
    "        print(f\"   ‚ö†Ô∏è Train perdi√≥ {len(train_imputed) - len(train_integrated)} registros\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Train mantuvo todos los registros\")\n",
    "    \n",
    "    if len(test_integrated) != len(test_imputed):\n",
    "        print(f\"   ‚ö†Ô∏è Test perdi√≥ {len(test_imputed) - len(test_integrated)} registros\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Test mantuvo todos los registros\")\n",
    "    \n",
    "    # Verificar nuevas variables\n",
    "    new_vars_train = set(train_integrated.columns) - set(train_imputed.columns)\n",
    "    print(f\"   üìä Nuevas variables agregadas: {len(new_vars_train)}\")\n",
    "    print(f\"   Variables: {list(new_vars_train)[:5]}...\")  # Mostrar primeras 5\n",
    "    \n",
    "    return train_integrated, test_integrated\n",
    "\n",
    "# Ejecutar integraci√≥n\n",
    "train_final, test_final = integrate_datasets_safely()\n",
    "\n",
    "print(\"\\n‚úÖ INTEGRACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Validaci√≥n de Calidad Final\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Validaci√≥n comprensiva de calidad\n",
    "def comprehensive_quality_check(train_df, test_df):\n",
    "    \"\"\"Realiza verificaci√≥n completa de calidad de datos.\"\"\"\n",
    "    \n",
    "    print(\"=== VALIDACI√ìN DE CALIDAD FINAL ===\")\n",
    "    \n",
    "    quality_report = {\n",
    "        'train': {\n",
    "            'shape': train_df.shape,\n",
    "            'missing_values': train_df.isnull().sum().sum(),\n",
    "            'duplicates': train_df.duplicated().sum(),\n",
    "            'data_types': train_df.dtypes.value_counts().to_dict()\n",
    "        },\n",
    "        'test': {\n",
    "            'shape': test_df.shape,\n",
    "            'missing_values': test_df.isnull().sum().sum(),\n",
    "            'duplicates': test_df.duplicated().sum(),\n",
    "            'data_types': test_df.dtypes.value_counts().to_dict()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Mostrar reporte\n",
    "    for dataset_name, metrics in quality_report.items():\n",
    "        print(f\"\\n{dataset_name.upper()}:\")\n",
    "        print(f\"  üìä Dimensiones: {metrics['shape'][0]:,} x {metrics['shape'][1]}\")\n",
    "        print(f\"  ‚ùì Valores faltantes: {metrics['missing_values']:,}\")\n",
    "        print(f\"  üîÑ Duplicados: {metrics['duplicates']:,}\")\n",
    "        print(f\"  üìã Tipos de datos: {metrics['data_types']}\")\n",
    "    \n",
    "    # Verificar target en train\n",
    "    if 'Target' in train_df.columns:\n",
    "        target_dist = train_df['Target'].value_counts()\n",
    "        target_nulls = train_df['Target'].isnull().sum()\n",
    "        print(f\"\\nüéØ VARIABLE TARGET:\")\n",
    "        print(f\"  Distribuci√≥n: {target_dist.to_dict()}\")\n",
    "        print(f\"  Valores nulos: {target_nulls}\")\n",
    "        \n",
    "        if target_nulls > 0:\n",
    "            print(\"  ‚ö†Ô∏è ALERTA: Target tiene valores nulos\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Target sin valores nulos\")\n",
    "    \n",
    "    # Verificar consistencia de columnas comunes\n",
    "    common_cols = set(train_df.columns) - {'Target', 'Retencion'}  # Excluir vars solo de train\n",
    "    test_cols = set(test_df.columns)\n",
    "    common_in_test = common_cols.intersection(test_cols)\n",
    "    \n",
    "    print(f\"\\nüîó CONSISTENCIA ENTRE DATASETS:\")\n",
    "    print(f\"  Columnas comunes: {len(common_in_test)} de {len(common_cols)}\")\n",
    "    \n",
    "    missing_in_test = common_cols - test_cols\n",
    "    if missing_in_test:\n",
    "        print(f\"  ‚ö†Ô∏è Faltan en test: {list(missing_in_test)[:3]}...\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Todas las columnas necesarias est√°n en test\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Ejecutar validaci√≥n\n",
    "quality_report = comprehensive_quality_check(train_final, test_final)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Exportaci√≥n de Datos Preparados\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Crear directorio de salida\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Guardar datasets preparados\n",
    "print(\"=== EXPORTACI√ìN DE DATOS PREPARADOS ===\")\n",
    "\n",
    "# Guardar train\n",
    "train_output_path = output_dir / \"train_cleaned_integrated.csv\"\n",
    "train_final.to_csv(train_output_path, index=False)\n",
    "print(f\"‚úÖ Train guardado: {train_output_path}\")\n",
    "print(f\"   {len(train_final):,} registros x {len(train_final.columns)} columnas\")\n",
    "\n",
    "# Guardar test\n",
    "test_output_path = output_dir / \"test_cleaned_integrated.csv\"\n",
    "test_final.to_csv(test_output_path, index=False)\n",
    "print(f\"‚úÖ Test guardado: {test_output_path}\")\n",
    "print(f\"   {len(test_final):,} registros x {len(test_final.columns)} columnas\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 8. Resumen Ejecutivo Final\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Generar resumen ejecutivo completo\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN EJECUTIVO - PREPARACI√ìN DE DATOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# M√©tricas de transformaci√≥n\n",
    "original_train_size = len(datasets['train'])\n",
    "final_train_size = len(train_final)\n",
    "original_cols = len(datasets['train'].columns)\n",
    "final_cols = len(train_final.columns)\n",
    "\n",
    "print(f\"\\nüìä TRANSFORMACI√ìN REALIZADA:\")\n",
    "print(f\"  Dataset Train: {original_train_size:,} ‚Üí {final_train_size:,} registros\")\n",
    "print(f\"  Variables: {original_cols} ‚Üí {final_cols} columnas (+{final_cols-original_cols})\")\n",
    "print(f\"  Integridad: {(final_train_size/original_train_size)*100:.1f}% registros conservados\")\n",
    "\n",
    "# Estado de completitud\n",
    "final_missing = train_final.isnull().sum().sum()\n",
    "total_cells = train_final.shape[0] * train_final.shape[1]\n",
    "completeness = ((total_cells - final_missing) / total_cells) * 100\n",
    "\n",
    "print(f\"\\nüìã CALIDAD FINAL:\")\n",
    "print(f\"  Completitud: {completeness:.2f}% ({final_missing:,} valores faltantes)\")\n",
    "print(f\"  Duplicados: {train_final.duplicated().sum():,} registros\")\n",
    "print(f\"  Variables num√©ricas: {len(train_final.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"  Variables categ√≥ricas: {len(train_final.select_dtypes(include=['object']).columns)}\")\n",
    "\n",
    "# Validaci√≥n de target\n",
    "if 'Target' in train_final.columns:\n",
    "    target_completeness = (1 - train_final['Target'].isnull().sum()/len(train_final)) * 100\n",
    "    target_balance = train_final['Target'].value_counts(normalize=True)\n",
    "    \n",
    "    print(f\"\\nüéØ VARIABLE TARGET:\")\n",
    "    print(f\"  Completitud: {target_completeness:.1f}%\")\n",
    "    print(f\"  Distribuci√≥n: {target_balance[0]:.1%} No Fuga, {target_balance[1]:.1%} Fuga\")\n",
    "    print(f\"  Desbalance: {target_balance[0]/target_balance[1]:.0f}:1\")\n",
    "\n",
    "print(f\"\\n‚úÖ PREPARACI√ìN DE DATOS COMPLETADA EXITOSAMENTE\")\n",
    "print(f\"üéØ Datos listos para Feature Engineering (Notebook 03)\")\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
