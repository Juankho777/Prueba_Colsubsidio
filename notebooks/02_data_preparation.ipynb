{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de Datos - Modelo de Fuga Colsubsidio\n",
    "# ====================================================\n",
    "# \n",
    "# Objetivo: Limpiar e integrar los datasets para crear una base sólida para el modelado\n",
    "# - Limpieza de variables financieras con formato de texto\n",
    "# - Manejo inteligente de valores faltantes\n",
    "# - Integración robusta de múltiples fuentes\n",
    "# - Validación de calidad post-limpieza\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Configuración Inicial y Carga de Datos\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Configuración inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Importar módulos del proyecto\n",
    "sys.path.append('..')\n",
    "from src.data_loader import DataLoader\n",
    "from src.preprocessing import DataPreprocessor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Librerías y módulos cargados exitosamente\")\n",
    "print(f\"Preparación iniciada: {pd.Timestamp.now()}\")\n",
    "\n",
    "# %%\n",
    "# Carga de datos usando módulo configurado\n",
    "data_loader = DataLoader()\n",
    "datasets = data_loader.load_all_datasets()\n",
    "\n",
    "# Referencias directas para facilitar trabajo\n",
    "train_raw = datasets['train'].copy()\n",
    "test_raw = datasets['test'].copy()\n",
    "demo_raw = datasets['demograficas'].copy()\n",
    "subs_raw = datasets['subsidios'].copy()\n",
    "\n",
    "print(\"=== DATASETS CARGADOS ===\")\n",
    "datasets_info = {\n",
    "    'Train': {'registros': len(train_raw), 'columnas': len(train_raw.columns)},\n",
    "    'Test': {'registros': len(test_raw), 'columnas': len(test_raw.columns)},\n",
    "    'Demográficas': {'registros': len(demo_raw), 'columnas': len(demo_raw.columns)},\n",
    "    'Subsidios': {'registros': len(subs_raw), 'columnas': len(subs_raw.columns)}\n",
    "}\n",
    "\n",
    "for name, info in datasets_info.items():\n",
    "    print(f\"{name}: {info['registros']:,} registros x {info['columnas']} columnas\")\n",
    "\n",
    "# Verificar estructura de columnas\n",
    "print(\"\\n=== VERIFICACIÓN DE ESTRUCTURA ===\")\n",
    "print(f\"Columnas en Train: {list(train_raw.columns)[:5]}...\")  # Primeras 5\n",
    "print(f\"Columnas en Test: {list(test_raw.columns)[:5]}...\")   # Primeras 5\n",
    "\n",
    "# Verificar variable target\n",
    "has_target = 'Target' in train_raw.columns\n",
    "print(f\"\\nVariable Target presente: {'✅' if has_target else '❌'}\")\n",
    "\n",
    "if has_target:\n",
    "    target_dist = train_raw['Target'].value_counts()\n",
    "    print(f\"Distribución Target: {target_dist.to_dict()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Limpieza de Variables Financieras\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Identificar variables financieras que necesitan limpieza\n",
    "financial_columns = [\n",
    "    'Disponible.Avances', 'Limite.Avances', 'Total.Intereses',\n",
    "    'Saldos.Mes.Ant', 'Pagos.Mes.Ant', 'Vtas.Mes.Ant',\n",
    "    'Limite.Cupo', 'Pago.del.Mes', 'Pago.Minimo',\n",
    "    'Vr.Mora', 'Vr.Cuota.Manejo', 'Saldo'\n",
    "]\n",
    "\n",
    "# Filtrar columnas que realmente existen\n",
    "available_financial = [col for col in financial_columns if col in train_raw.columns]\n",
    "\n",
    "print(f\"Variables financieras a limpiar: {len(available_financial)}\")\n",
    "print(f\"Variables: {available_financial[:5]}...\")  # Mostrar primeras 5\n",
    "\n",
    "# %%\n",
    "# Función de limpieza personalizada\n",
    "def clean_financial_variable(series, column_name):\n",
    "    \"\"\"Limpia una variable financiera específica.\"\"\"\n",
    "    print(f\"\\nLimpiando {column_name}...\")\n",
    "    \n",
    "    # Estadísticas antes de limpieza\n",
    "    original_type = series.dtype\n",
    "    original_nulls = series.isnull().sum()\n",
    "    \n",
    "    # Proceso de limpieza\n",
    "    cleaned = series.astype(str)\n",
    "    \n",
    "    # Remover caracteres comunes en formato monetario\n",
    "    replacements = [',', '$', ' ', '.00']\n",
    "    for char in replacements:\n",
    "        cleaned = cleaned.str.replace(char, '')\n",
    "    \n",
    "    # Manejar valores especiales\n",
    "    cleaned = cleaned.replace(['nan', 'NaN', 'None', ''], np.nan)\n",
    "    \n",
    "    # Convertir a numérico\n",
    "    cleaned_numeric = pd.to_numeric(cleaned, errors='coerce')\n",
    "    \n",
    "    # Estadísticas después de limpieza\n",
    "    final_nulls = cleaned_numeric.isnull().sum()\n",
    "    new_nulls = final_nulls - original_nulls\n",
    "    \n",
    "    print(f\"  Tipo original: {original_type} → float64\")\n",
    "    print(f\"  NaN originales: {original_nulls:,} → finales: {final_nulls:,} (+{new_nulls:,})\")\n",
    "    \n",
    "    if len(cleaned_numeric.dropna()) > 0:\n",
    "        print(f\"  Rango: ${cleaned_numeric.min():,.0f} - ${cleaned_numeric.max():,.0f}\")\n",
    "    \n",
    "    return cleaned_numeric\n",
    "\n",
    "# Aplicar limpieza a train\n",
    "print(\"=== LIMPIEZA DATASET TRAIN ===\")\n",
    "train_clean = train_raw.copy()\n",
    "\n",
    "for col in available_financial:\n",
    "    if col in train_clean.columns:\n",
    "        train_clean[col] = clean_financial_variable(train_clean[col], col)\n",
    "\n",
    "# Aplicar limpieza a test\n",
    "print(\"\\n=== LIMPIEZA DATASET TEST ===\")\n",
    "test_clean = test_raw.copy()\n",
    "\n",
    "for col in available_financial:\n",
    "    if col in test_clean.columns:\n",
    "        test_clean[col] = clean_financial_variable(test_clean[col], col)\n",
    "\n",
    "print(\"\\n✅ Limpieza de variables financieras completada\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Análisis de Valores Faltantes Post-Limpieza\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Análisis completo de valores faltantes\n",
    "def analyze_missing_values(df, dataset_name):\n",
    "    \"\"\"Analiza patrones de valores faltantes.\"\"\"\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_pct = (missing_data / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Variable': missing_data.index,\n",
    "        'Faltantes': missing_data.values,\n",
    "        'Porcentaje': missing_pct.values\n",
    "    }).query('Faltantes > 0').sort_values('Porcentaje', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== ANÁLISIS DE FALTANTES - {dataset_name.upper()} ===\")\n",
    "    print(f\"Variables con faltantes: {len(missing_df)} de {len(df.columns)}\")\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(\"\\nTop 10 variables con más faltantes:\")\n",
    "        for _, row in missing_df.head(10).iterrows():\n",
    "            print(f\"  {row['Variable']}: {row['Faltantes']:,} ({row['Porcentaje']:.1f}%)\")\n",
    "        \n",
    "        # Categorizar por severidad\n",
    "        severe = missing_df[missing_df['Porcentaje'] > 90]\n",
    "        high = missing_df[(missing_df['Porcentaje'] > 50) & (missing_df['Porcentaje'] <= 90)]\n",
    "        medium = missing_df[(missing_df['Porcentaje'] > 10) & (missing_df['Porcentaje'] <= 50)]\n",
    "        low = missing_df[missing_df['Porcentaje'] <= 10]\n",
    "        \n",
    "        print(f\"\\nCategorización por severidad:\")\n",
    "        print(f\"  Severo (>90%): {len(severe)} variables\")\n",
    "        print(f\"  Alto (50-90%): {len(high)} variables\")\n",
    "        print(f\"  Medio (10-50%): {len(medium)} variables\")\n",
    "        print(f\"  Bajo (<10%): {len(low)} variables\")\n",
    "        \n",
    "        return missing_df\n",
    "    else:\n",
    "        print(\"✅ No hay valores faltantes\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Analizar train y test\n",
    "missing_train = analyze_missing_values(train_clean, 'train')\n",
    "missing_test = analyze_missing_values(test_clean, 'test')\n",
    "\n",
    "# %%\n",
    "# Gráfico comparativo de faltantes\n",
    "if len(missing_train) > 0 or len(missing_test) > 0:\n",
    "    # Combinar datos para visualización\n",
    "    fig_missing = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=['Train Dataset', 'Test Dataset'],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    if len(missing_train) > 0:\n",
    "        top_missing_train = missing_train.head(10)\n",
    "        fig_missing.add_trace(\n",
    "            go.Bar(\n",
    "                y=top_missing_train['Variable'],\n",
    "                x=top_missing_train['Porcentaje'],\n",
    "                name='Train',\n",
    "                orientation='h',\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Test\n",
    "    if len(missing_test) > 0:\n",
    "        top_missing_test = missing_test.head(10)\n",
    "        fig_missing.add_trace(\n",
    "            go.Bar(\n",
    "                y=top_missing_test['Variable'],\n",
    "                x=top_missing_test['Porcentaje'],\n",
    "                name='Test',\n",
    "                orientation='h',\n",
    "                marker_color='lightcoral'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig_missing.update_layout(\n",
    "        title_text=\"Comparación de Valores Faltantes - Train vs Test\",\n",
    "        title_font_size=16,\n",
    "        height=600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig_missing.update_xaxes(title_text=\"Porcentaje Faltante (%)\")\n",
    "    fig_missing.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Estrategia de Imputación\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Estrategia de imputación inteligente\n",
    "def apply_missing_strategy(df, dataset_name):\n",
    "    \"\"\"Aplica estrategia específica según el tipo de variable.\"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    print(f\"\\n=== ESTRATEGIA DE IMPUTACIÓN - {dataset_name.upper()} ===\")\n",
    "    \n",
    "    # 1. Variables financieras: rellenar con 0 (sentido de negocio)\n",
    "    financial_vars = [col for col in available_financial if col in df_imputed.columns]\n",
    "    if financial_vars:\n",
    "        for col in financial_vars:\n",
    "            nulls_before = df_imputed[col].isnull().sum()\n",
    "            df_imputed[col] = df_imputed[col].fillna(0)\n",
    "            print(f\"  {col}: {nulls_before:,} NaN → 0 (lógica financiera)\")\n",
    "    \n",
    "    # 2. Variables categóricas: rellenar con 'Unknown'\n",
    "    categorical_vars = df_imputed.select_dtypes(include=['object']).columns\n",
    "    categorical_vars = [col for col in categorical_vars if col not in ['id']]\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        nulls_before = df_imputed[col].isnull().sum()\n",
    "        if nulls_before > 0:\n",
    "            df_imputed[col] = df_imputed[col].fillna('Unknown')\n",
    "            print(f\"  {col}: {nulls_before:,} NaN → 'Unknown'\")\n",
    "    \n",
    "    # 3. Variables numéricas restantes: usar mediana\n",
    "    numeric_vars = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "    numeric_vars = [col for col in numeric_vars if col not in financial_vars + ['id', 'Target']]\n",
    "    \n",
    "    for col in numeric_vars:\n",
    "        nulls_before = df_imputed[col].isnull().sum()\n",
    "        if nulls_before > 0:\n",
    "            median_val = df_imputed[col].median()\n",
    "            df_imputed[col] = df_imputed[col].fillna(median_val)\n",
    "            print(f\"  {col}: {nulls_before:,} NaN → {median_val:.1f} (mediana)\")\n",
    "    \n",
    "    # Verificar resultado\n",
    "    remaining_nulls = df_imputed.isnull().sum().sum()\n",
    "    print(f\"\\nValores faltantes restantes: {remaining_nulls:,}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Aplicar estrategia de imputación\n",
    "train_imputed = apply_missing_strategy(train_clean, 'train')\n",
    "test_imputed = apply_missing_strategy(test_clean, 'test')\n",
    "\n",
    "print(\"\\n✅ Imputación de valores faltantes completada\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Integración de Datasets\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Verificar consistencia de IDs antes del merge\n",
    "def verify_id_consistency():\n",
    "    \"\"\"Verifica que los IDs sean consistentes entre datasets.\"\"\"\n",
    "    \n",
    "    train_ids = set(train_imputed['id'].unique())\n",
    "    test_ids = set(test_imputed['id'].unique())\n",
    "    demo_ids = set(demo_raw['id'].unique())\n",
    "    subs_ids = set(subs_raw['id'].unique())\n",
    "    \n",
    "    print(\"=== VERIFICACIÓN DE CONSISTENCIA DE IDs ===\")\n",
    "    print(f\"IDs únicos Train: {len(train_ids):,}\")\n",
    "    print(f\"IDs únicos Test: {len(test_ids):,}\")\n",
    "    print(f\"IDs únicos Demográficas: {len(demo_ids):,}\")\n",
    "    print(f\"IDs únicos Subsidios: {len(subs_ids):,}\")\n",
    "    \n",
    "    # Verificar solapamientos\n",
    "    all_main_ids = train_ids.union(test_ids)\n",
    "    demo_coverage = len(all_main_ids.intersection(demo_ids)) / len(all_main_ids)\n",
    "    subs_coverage = len(all_main_ids.intersection(subs_ids)) / len(all_main_ids)\n",
    "    \n",
    "    print(f\"\\nCobertura Demográficas: {demo_coverage:.1%}\")\n",
    "    print(f\"Cobertura Subsidios: {subs_coverage:.1%}\")\n",
    "    \n",
    "    # Verificar separación train/test\n",
    "    overlap = train_ids.intersection(test_ids)\n",
    "    print(f\"\\nSolapamiento Train-Test: {len(overlap)} IDs\")\n",
    "    \n",
    "    if len(overlap) > 0:\n",
    "        print(\"⚠️ ALERTA: Hay solapamiento entre train y test\")\n",
    "    else:\n",
    "        print(\"✅ Train y test están correctamente separados\")\n",
    "    \n",
    "    return {\n",
    "        'demo_coverage': demo_coverage,\n",
    "        'subs_coverage': subs_coverage,\n",
    "        'train_test_overlap': len(overlap)\n",
    "    }\n",
    "\n",
    "id_verification = verify_id_consistency()\n",
    "\n",
    "# %%\n",
    "# Proceso de integración con validaciones\n",
    "def integrate_datasets_safely():\n",
    "    \"\"\"Integra datasets con validaciones de calidad.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== PROCESO DE INTEGRACIÓN ===\")\n",
    "    \n",
    "    # Merge train con demográficas\n",
    "    print(\"\\n1. Integrando TRAIN con DEMOGRÁFICAS...\")\n",
    "    train_demo = train_imputed.merge(\n",
    "        demo_raw, \n",
    "        on='id', \n",
    "        how='left', \n",
    "        suffixes=('', '_demo')\n",
    "    )\n",
    "    \n",
    "    print(f\"   Antes: {len(train_imputed):,} registros\")\n",
    "    print(f\"   Después: {len(train_demo):,} registros\")\n",
    "    print(f\"   Nuevas columnas: {len(train_demo.columns) - len(train_imputed.columns)}\")\n",
    "    \n",
    "    # Merge con subsidios\n",
    "    print(\"\\n2. Integrando con SUBSIDIOS...\")\n",
    "    train_integrated = train_demo.merge(\n",
    "        subs_raw,\n",
    "        on='id',\n",
    "        how='left',\n",
    "        suffixes=('', '_subs')\n",
    "    )\n",
    "    \n",
    "    print(f\"   Antes: {len(train_demo):,} registros\")\n",
    "    print(f\"   Después: {len(train_integrated):,} registros\")\n",
    "    print(f\"   Columnas finales: {len(train_integrated.columns)}\")\n",
    "    \n",
    "    # Mismo proceso para test\n",
    "    print(\"\\n3. Integrando TEST...\")\n",
    "    test_demo = test_imputed.merge(\n",
    "        demo_raw,\n",
    "        on='id',\n",
    "        how='left',\n",
    "        suffixes=('', '_demo')\n",
    "    )\n",
    "    \n",
    "    test_integrated = test_demo.merge(\n",
    "        subs_raw,\n",
    "        on='id', \n",
    "        how='left',\n",
    "        suffixes=('', '_subs')\n",
    "    )\n",
    "    \n",
    "    print(f\"   Test final: {len(test_integrated):,} registros x {len(test_integrated.columns)} columnas\")\n",
    "    \n",
    "    # Validaciones post-merge\n",
    "    print(\"\\n4. Validaciones post-integración...\")\n",
    "    \n",
    "    # Verificar pérdida de registros\n",
    "    if len(train_integrated) != len(train_imputed):\n",
    "        print(f\"   ⚠️ Train perdió {len(train_imputed) - len(train_integrated)} registros\")\n",
    "    else:\n",
    "        print(\"   ✅ Train mantuvo todos los registros\")\n",
    "    \n",
    "    if len(test_integrated) != len(test_imputed):\n",
    "        print(f\"   ⚠️ Test perdió {len(test_imputed) - len(test_integrated)} registros\")\n",
    "    else:\n",
    "        print(\"   ✅ Test mantuvo todos los registros\")\n",
    "    \n",
    "    # Verificar nuevas variables\n",
    "    new_vars_train = set(train_integrated.columns) - set(train_imputed.columns)\n",
    "    print(f\"   📊 Nuevas variables agregadas: {len(new_vars_train)}\")\n",
    "    print(f\"   Variables: {list(new_vars_train)[:5]}...\")  # Mostrar primeras 5\n",
    "    \n",
    "    return train_integrated, test_integrated\n",
    "\n",
    "# Ejecutar integración\n",
    "train_final, test_final = integrate_datasets_safely()\n",
    "\n",
    "print(\"\\n✅ INTEGRACIÓN COMPLETADA EXITOSAMENTE\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Validación de Calidad Final\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Validación comprensiva de calidad\n",
    "def comprehensive_quality_check(train_df, test_df):\n",
    "    \"\"\"Realiza verificación completa de calidad de datos.\"\"\"\n",
    "    \n",
    "    print(\"=== VALIDACIÓN DE CALIDAD FINAL ===\")\n",
    "    \n",
    "    quality_report = {\n",
    "        'train': {\n",
    "            'shape': train_df.shape,\n",
    "            'missing_values': train_df.isnull().sum().sum(),\n",
    "            'duplicates': train_df.duplicated().sum(),\n",
    "            'data_types': train_df.dtypes.value_counts().to_dict()\n",
    "        },\n",
    "        'test': {\n",
    "            'shape': test_df.shape,\n",
    "            'missing_values': test_df.isnull().sum().sum(),\n",
    "            'duplicates': test_df.duplicated().sum(),\n",
    "            'data_types': test_df.dtypes.value_counts().to_dict()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Mostrar reporte\n",
    "    for dataset_name, metrics in quality_report.items():\n",
    "        print(f\"\\n{dataset_name.upper()}:\")\n",
    "        print(f\"  📊 Dimensiones: {metrics['shape'][0]:,} x {metrics['shape'][1]}\")\n",
    "        print(f\"  ❓ Valores faltantes: {metrics['missing_values']:,}\")\n",
    "        print(f\"  🔄 Duplicados: {metrics['duplicates']:,}\")\n",
    "        print(f\"  📋 Tipos de datos: {metrics['data_types']}\")\n",
    "    \n",
    "    # Verificar target en train\n",
    "    if 'Target' in train_df.columns:\n",
    "        target_dist = train_df['Target'].value_counts()\n",
    "        target_nulls = train_df['Target'].isnull().sum()\n",
    "        print(f\"\\n🎯 VARIABLE TARGET:\")\n",
    "        print(f\"  Distribución: {target_dist.to_dict()}\")\n",
    "        print(f\"  Valores nulos: {target_nulls}\")\n",
    "        \n",
    "        if target_nulls > 0:\n",
    "            print(\"  ⚠️ ALERTA: Target tiene valores nulos\")\n",
    "        else:\n",
    "            print(\"  ✅ Target sin valores nulos\")\n",
    "    \n",
    "    # Verificar consistencia de columnas comunes\n",
    "    common_cols = set(train_df.columns) - {'Target', 'Retencion'}  # Excluir vars solo de train\n",
    "    test_cols = set(test_df.columns)\n",
    "    common_in_test = common_cols.intersection(test_cols)\n",
    "    \n",
    "    print(f\"\\n🔗 CONSISTENCIA ENTRE DATASETS:\")\n",
    "    print(f\"  Columnas comunes: {len(common_in_test)} de {len(common_cols)}\")\n",
    "    \n",
    "    missing_in_test = common_cols - test_cols\n",
    "    if missing_in_test:\n",
    "        print(f\"  ⚠️ Faltan en test: {list(missing_in_test)[:3]}...\")\n",
    "    else:\n",
    "        print(f\"  ✅ Todas las columnas necesarias están en test\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Ejecutar validación\n",
    "quality_report = comprehensive_quality_check(train_final, test_final)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Exportación de Datos Preparados\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Crear directorio de salida\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Guardar datasets preparados\n",
    "print(\"=== EXPORTACIÓN DE DATOS PREPARADOS ===\")\n",
    "\n",
    "# Guardar train\n",
    "train_output_path = output_dir / \"train_cleaned_integrated.csv\"\n",
    "train_final.to_csv(train_output_path, index=False)\n",
    "print(f\"✅ Train guardado: {train_output_path}\")\n",
    "print(f\"   {len(train_final):,} registros x {len(train_final.columns)} columnas\")\n",
    "\n",
    "# Guardar test\n",
    "test_output_path = output_dir / \"test_cleaned_integrated.csv\"\n",
    "test_final.to_csv(test_output_path, index=False)\n",
    "print(f\"✅ Test guardado: {test_output_path}\")\n",
    "print(f\"   {len(test_final):,} registros x {len(test_final.columns)} columnas\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 8. Resumen Ejecutivo Final\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Generar resumen ejecutivo completo\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN EJECUTIVO - PREPARACIÓN DE DATOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Métricas de transformación\n",
    "original_train_size = len(datasets['train'])\n",
    "final_train_size = len(train_final)\n",
    "original_cols = len(datasets['train'].columns)\n",
    "final_cols = len(train_final.columns)\n",
    "\n",
    "print(f\"\\n📊 TRANSFORMACIÓN REALIZADA:\")\n",
    "print(f\"  Dataset Train: {original_train_size:,} → {final_train_size:,} registros\")\n",
    "print(f\"  Variables: {original_cols} → {final_cols} columnas (+{final_cols-original_cols})\")\n",
    "print(f\"  Integridad: {(final_train_size/original_train_size)*100:.1f}% registros conservados\")\n",
    "\n",
    "# Estado de completitud\n",
    "final_missing = train_final.isnull().sum().sum()\n",
    "total_cells = train_final.shape[0] * train_final.shape[1]\n",
    "completeness = ((total_cells - final_missing) / total_cells) * 100\n",
    "\n",
    "print(f\"\\n📋 CALIDAD FINAL:\")\n",
    "print(f\"  Completitud: {completeness:.2f}% ({final_missing:,} valores faltantes)\")\n",
    "print(f\"  Duplicados: {train_final.duplicated().sum():,} registros\")\n",
    "print(f\"  Variables numéricas: {len(train_final.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"  Variables categóricas: {len(train_final.select_dtypes(include=['object']).columns)}\")\n",
    "\n",
    "# Validación de target\n",
    "if 'Target' in train_final.columns:\n",
    "    target_completeness = (1 - train_final['Target'].isnull().sum()/len(train_final)) * 100\n",
    "    target_balance = train_final['Target'].value_counts(normalize=True)\n",
    "    \n",
    "    print(f\"\\n🎯 VARIABLE TARGET:\")\n",
    "    print(f\"  Completitud: {target_completeness:.1f}%\")\n",
    "    print(f\"  Distribución: {target_balance[0]:.1%} No Fuga, {target_balance[1]:.1%} Fuga\")\n",
    "    print(f\"  Desbalance: {target_balance[0]/target_balance[1]:.0f}:1\")\n",
    "\n",
    "print(f\"\\n✅ PREPARACIÓN DE DATOS COMPLETADA EXITOSAMENTE\")\n",
    "print(f\"🎯 Datos listos para Feature Engineering (Notebook 03)\")\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
